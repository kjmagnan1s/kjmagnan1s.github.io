[
  {
    "path": "posts/2024-05-31-Chicago-Summer-Violence-Analysis/",
    "title": "Chicago Summer Violence Analysis",
    "description": "A detailed overview of my Chicago Summer Violence Dashboard, highlighting its key features, data sources, and practical applications for enhancing public safety in Chicago.\n\nEstimated Read Time: 6 minutes",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2024-05-31",
    "categories": [
      "Tableau",
      "data visualization",
      "GenAI",
      "GIS",
      "interactive",
      "inspiration"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nHexagon Grids for Spatial Analysis\r\nChicago Violence Reduction Dashboard Data\r\nChicago Summer Violence Tableau Dashboard\r\nConclusion\r\nAcknowledgments\r\n\r\nIntroduction\r\nIn today’s justice and public safety landscape, data-driven decision-making is essential for addressing complex challenges. Recently, I was inspired by the University of Chicago Crime Lab’s Chicago Summer Violence analysis and interactive map, which showcases an innovative approach to public safety issues. Motivated by their work and leveraging data from the City of Chicago Violence Reduction Dashboard, I created my own version of the dashboard with some improvements and customizations I have found in my career to be instrumental in conveying critical and timely public safety data to practitioners. I’m excited to introduce my Chicago Summer Violence Dashboard, highlighting its features, data sources, and its potential impact on violence reduction strategies.\r\nWhat caught my attention with the Crime Lab’s Summer Violence map was their use of a raster grid analysis overlaid on top of Chicago, a practical method to group points across a large city like Chicago. This method is particularly effective due to the political and socioeconomic nature of traditional, antiquated geographies in the United States. Police boundaries, neighborhoods, and even political districts rarely change, and often do not change through a rigorous data-driven process. Therefore, uniform raster grids provide a structured, yet effective GIS mapping technique.\r\nThat being said, the particular 45-degree shifted square grid used by Crime Lab left me wanting more. Instead of a traditional square raster grid application, I made use of hexagons to capture both the staggering of latitude and longitude points from the Chicago Data Portal (done to ‘randomize’ exact locations of shootings) and eliminate the sharp edges and potential abnormalities from the 45-degree angled square used by the Crime Lab, which may not properly conform to street segments in Chicago.\r\nHexagon Grids for Spatial Analysis\r\nIn creating my Chicago Summer Violence Dashboard, I relied heavily on a fantastic tutorial by Sarah Battersby (twitter) detailed in a blog post on the Tableau Community site, titled How to Create Hexagonal Grids for Spatial Aggregation in Tableau This tutorial provided a clear and comprehensive guide on generating hexagonal grids using QGIS, which I then integrated into Tableau for spatial analysis.\r\nThe only change I made from Sarah’s tutorial was the sizing of the hexagon grid to better reflect the City of Chicago and arrange better within Chicago Community Areas and CPD Districts.\r\nExample of hexagon gidImportant note for introducing the hexagon grids into Tableau is to make use of the grid ID. Spatial Joins in Tableau, while possible, can significantly degrade dashboard performance. As a result, I built the hexagon grids and performed spatial joins for both the City of Chicago Shooting data and ShotSpotter data in QGIS.\r\nChicago Violence Reduction Dashboard Data\r\nChicago Violence Reduction Dashboard Data PortalThe Chicago Data Portal offers a variety of data sources from city organizations, including public safety and the Chicago Police Department (CPD). For this analysis, I utilized the Victims of Homicides and Non-Fatal Shootings and ShotSpotter Alerts datasets. While the Crime Lab’s analysis took a different approach by incorporating robbery data from the city’s larger crime dataset, I believe a more focused approach on weapon-related or firearm-related incidents would have been beneficial for their analysis.\r\nThat being said,I placed greater emphasis on ShotSpotter data, not only due to the recent discussions about its future in Chicago but also because of my prior experience working with this data. ShotSpotter has proven effective in identifying areas with high levels of gunfire, regardless of whether injuries occurred or 9-1-1 calls were made. This makes it a valuable resource for pinpointing locations that require immediate attention and intervention.\r\nChicago Summer Violence Tableau Dashboard\r\nIn Tableau Public Desktop, I loaded the hexagon GeoJSON file and created inner joins with three boundary layers: Community Areas, Police Beats, and Police Districts. These joins were based on the hexagon grid ID, which I already spatially joined in QGIS to avoid less efficient spatial joins within Tableau. After setting up the boundaries, I established relationships on the hexagon grid ID between the boundaries and the two datasets from the Chicago Data Portal.\r\nGIS Boundary JoinsData RelationshipsOnce the data was loaded, the next step was building the dashboard. While I won’t delve into the dashboard design details here (potentially saving that for another post), the unique aspect of this analysis was working with the hexagon grids and mapping layers in Tableau. Visualizing the grids works similarly to visualizing any mapping layer through the geometry and generated latitude and longitude of the polygons. The hexagon unique ID is crucial for Tableau to identify each unique grid and associate shootings and ShotSpotter alerts with those grids, enabling the creation of choropleths and accurate counts.\r\nParameters are invaluable for allowing users to enable or disable each layer within the dashboard. For example, when a user selects the Police Districts map layer, a Tableau parameter combined with a calculated field isolates this selection, preventing the geometry data of other layers from being visualized.\r\nTableau Map LayersI also leveraged Tableau’s dynamic visibility feature to swap the temporal visualizations between Shooting and ShotSpotter data based on the dashboard parameter. This allows users to seamlessly switch between different datasets without cluttering the interface. By using dynamic visibility, I was able to maintain a clean and user-friendly dashboard that adapts to the viewer’s needs, making it easier to compare and analyze different types of data or data from separate data sources.\r\nThe final technical complexity involved determining the top x% of hexagon grids across Chicago for shootings and ShotSpotter alerts. I’ll keep this brief, but the calculated fields can be found in my Tableau Public Dashboard. In summary, for both shootings and ShotSpotter alerts, I calculated the counts for each grid and uniquely ranked them. Simultaneously, I allowed users to input any percentage of top hexagon grids to retain, offering a variation from Crime Lab’s fixed 5% or 10% selection. I then calculated the number of grids this user selection represented. Finally, I created a calculation for Tableau to retain only the number of ranked grids corresponding to the user selection. Thus, if a user selected a top percentage that resulted in 500 grids, Tableau would only visualize the top 500 grids ranked for either shootings or ShotSpotter alerts.\r\nYou can find the dashboard on my Tableau Public page or embedded below:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nConclusion\r\nBuilding the Chicago Summer Violence Dashboard was both a challenging and rewarding experience. By using hexagon grids and Tableau’s advanced mapping and dynamic visibility features, I aimed to create a tool that provides clear and actionable insights into violence trends across the city.\r\nI hope you find this dashboard useful and easy to navigate. Feel free to explore the data, customize the views, and share your insights. Your feedback is always welcome as I continue to refine and improve this tool.\r\nAcknowledgments\r\nThis blog post was structured and written with the assistance of OpenAI’s ChatGPT through one of my customGPT’s. These tools helped streamline the writing process, making it more clear and concise for all audiences. I also leveraged the new GPT 4-o model in dashboard design, accessibility, and analysis. Utilizing advanced AI technologies allowed me to focus more on the insights and technical aspects of the dashboard creation, ensuring that the information presented is accessible and engaging for readers.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-05-31-Chicago-Summer-Violence-Analysis/preview.png",
    "last_modified": "2025-02-14T15:53:34-06:00",
    "input_file": {},
    "preview_width": 1653,
    "preview_height": 1168
  },
  {
    "path": "posts/2022-01-02-TC21-blog-slalom/",
    "title": "Tableau Conference 2021 Blog in Partnership with Slalom",
    "description": "Check out this blog I wrote in partnership with Slalom Consulting and a colleague of mine at the company. Hopefully TC22 will return to in person so we can truly experience a TC event!\n\nhttps://www.slalom.com/insight/data-viz-trends-2022",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2022-01-01",
    "categories": [
      "blog",
      "Tableau",
      "data visualization",
      "Conference"
    ],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-02-TC21-blog-slalom/thumbnail.png",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {},
    "preview_width": 320,
    "preview_height": 167
  },
  {
    "path": "posts/2021-03-31-Tufte -mini-blog-1/",
    "title": "Lessons learned from Tufte: Sparklines and datawords",
    "description": "This mini-blog series will explore real-world examples of my interpretation and inclusion of design theories and examples from Edward Tufte.\n\nIn this post, I explore a familiar topic of combining text, numbers, and data lines into large-scale visualization",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2021-04-05",
    "categories": [
      "mini-blog",
      "Tableau",
      "data visualization",
      "Tufte"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nSparklines and Datawords\r\nBuilding a sparkline and dataword dashboard\r\n\r\nBackground\r\nIn my pursuit of professional growth and skill development, I have recently turned to design theory, visualization best practices, and authors who are held in high regard for these principles. One such author I have particularly enjoyed is Edward Tufte. After reading “The Visual Display of Quantitative Information”, I was left with a dozen or so bookmarks of charts and visualization examples to replicate or incorporate into future work. Leveraging my backing in Tableau and R (& my growing skills in Power BI) and a wealth of data viz opportunities from community challenges including #MakeoverMonday, #RWFD, and #TidyTuesday, I thought this would be a perfect excuse to incorporate the lessons learned from Tufte into my work, blog about the development of visualizations, and reflect on the practicality and use-case of each particular lesson from Tufte (and others!).\r\nTime to get to work!Sparklines and Datawords\r\nFor the seasoned data analyst or data visualizer out there, I will venture to guess the concept of sparklines and datawords will not be a foreign concept. While the term Tufte uses may be new, this type of visual goes by many names (for example callout numbers, callout boxes, dashboard cards, visual annotations) and is ubiquitous with business dashboards and sporting visuals, to name a few. What I found compelling from Tufte’s writing was the extent to which these visuals can derive meaning, capitalize on context, and push the boundaries of data presented within a relatively small footprint. I’ll go into detail on what I mean by this in the next section.\r\nAt its core, sparklines and datawords are, as Tufte puts it, “data-intense, design simple, word-sized graphics”. Sparklines and datawords can simply compliment a textbox or act as standalone visual to call attention to a single measurement, but they can also be used as a comprehensive visualization in the form of a small multiples visual or combined with a large table of data.\r\nExample of a sparkline and dataword from Twitter analyticsExample of a sparkline and dataword from Tufte’s Visual Display of Quantitative InformationWhen I encountered this concept in Tufte’s book, I knew I wanted to capitalize on the effectiveness of sparklines and design an entire dashboard around this idea. Fortunately, #MakeoverMonday’s weekly challenge covered global cash crops and the dataset provided had a variety of contextually relevant and interconnected dimensions. Additionally, Tufte provided an example of a visualization on forecasting economic indices in his book (page 180) with a template which I found to be perfect for this week’s #MM challenge.\r\nDataword visual inspiration - TufteAfter close examination, this visual has a few important characteristics I plan to replicate in a Tableau dashboard. Namely:\r\nSome kind of baseline or average value/dataword established across all dimensions (i.e. the black panel)\r\nColumns of datawords arranged ascending/descending based on the baseline value - in this example, based on how close they were to the baseline forecast\r\nStandardized containers for each dimension and value across the visual\r\nDescriptions or legends placed organically in blank areas caused by the arrangement of the datawords\r\nBuilding a sparkline and dataword dashboard\r\nUp to this point, I have covered the why, the what, and the so what; now we will cover the how.\r\nTime to Wubbalubbadubdub (Image source: tvfanatic.com)Taking advantage of the global cash crops dataset posted to dataworld for #MakeoverMonday, we can begin building this dashboard, making sure to incorporate as many of the observations from the visual above.\r\nThe global cash crops dataset provides an advantageous structuring of data for our visualization. It contains a region column for 5 of the 6 continents (excluding Antarctica) plus a global category, an element column describing the type of crop resource measured, a year column, and a value column (including a unit of measure). Thus, we can arrange the columns based on element (crop production, harvest, and yield) broken up by region, and described by year and value/unit. Additionally, while no baseline or target to compare regions was provided, we can compare each region against the global value (more importantly, against the global percent change). Furthermore, since each visual is standardized in its size, visualization, and filters, with small variation to the dataset dimensional values, we need only create a template and replicate it across the dashboard.\r\nStarting with the template, I will need one sheet with a dynamic text box and value for the dataword and one sheet for the sparkline. The dynamic text box is simply the ‘element’, ‘area’, and ‘value’ dimensions - though the ‘value’ is a percent change of the current year from the overall average, respective of the ‘area’ and ‘element’.\r\nDataword templateWith the easy part out of the way, I can now move on to the sparkline visual. You’ll notice the example Tufte provided in his book was simply an arrangement of datawords. I am much more interested in including both the dataword and sparkline as the sparkline offers a historical, time-series context that should be useful with this dataset.\r\nThe sparkline visual I went for is a simple line chart with a large end-point dot and a 95% confidence interval band. The pronounced dot at the end of the line signifies the last date value of the line chart and also helps the viewer evaluate each respective line chart’s historical trend as well as compare across sparklines of the same dimension(s). The confidence interval, on the other hand, provides this incredibly powerful yet simplistic visualization of a normal range of values and the reader can quickyl evaluate whether a year or time period has performed above or below that range.\r\nThe end-point dot ‘trick’ is simple to accomplish in Tableau by utilizing the dual-axis functionality of a sheet (I plan on learning how to do something like this in PowerBI very soon!). Arguably the most difficult part of this viz is identifying the last point in the line chart. There are numerous ways to write this calculation but I found this IIF (i.e. if and only if) table calc to work best and most succinctly.\r\nIIF(LAST()=0, SUM([Value]), NULL)\r\nOnce the dual axes are combined and synchronized, you should end up with a sparkline template like this:\r\nSparkline templateWith the dataword and sparkline templates completed, the tedious work begins. This viz should end up having 36 separate sheets (3 rows of elements, 6 regions, and 2 vizzes for each region - dataword and sparkine) based on the original dataword and sparkline templates. The final touches for my viz included a title, a description, an interactive legend, or how to read section, and a footer with my Twitter signature and data source. In keeping with Tufte’s example, I added a background to the worldwide dataword and sparkline visualizations across all 3 element dimensions to signify the baseline established by the global values.\r\nHere is the final product:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n That’s going to finish up my first blog post of this mini-series! I will be following up with a second post on a dot-dash-plot very soon!\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-31-Tufte -mini-blog-1/preview.svg",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-11-tableau-interactive-resume/",
    "title": "Tableau Interactive Resume",
    "description": "A walkthrough on building an interactive resume to show your Tableau skills and also serve as a comprehensive resume of your skills, experience, and background.",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2021-03-11",
    "categories": [
      "Tableau",
      "data visualization",
      "Figma",
      "interactive",
      "inspiration"
    ],
    "contents": "\r\n\r\nContents\r\nBackground\r\nPlanning New Dashboard\r\nDashboard Background Design with Figma\r\nData Prep\r\nBuild & Publish\r\nMarch 2021 Update:\r\n\r\nBackground\r\nThe idea of a Tableau interactive resume was not a new concept to me; I first published my version of it back in June of 2019. At that time, I was still learning a lot about Tableau concepts, visualization best practices, and was certainly still ‘trapped’ within a rigid framework of how to build a dashboard. As I reflected on a new version of my interactive resume in 2021, I appreciated my mindset back in 2019 while, at the same time, identifying numerous areas for improvement and opportunities to incorporate more advanced Tableau techniques I have cultivated over the years and following the work of others in the #DataFam Tableau Public community.\r\nMy first Tableau interactive resume, circa 2019I hope that after reading this blog, you will feel empowered and motivated to explore and develop your interactive resume and share it with the world!\r\n\r\n\r\n\r\nvia GIPHY\r\n\r\nPlanning New Dashboard\r\nThe first place to start when considering how to build and visualize your resume is on Tableau Public. I am a huge advocate for not reinventing the wheel and, instead, leveraging and building off of the incredible wealth of examples built by the Tableau community (with proper inspiration/credit given, of course!). With that in mind, starting with a review of Tableau’s Interactive Resume Gallery here is the best recommendation I can give for Tableau experts and newcomers, alike.\r\nFrom there, I also found Google searches for ‘interactive resume’ or ‘visual resume’ helpful ways to explore other resume configurations, opportunities for interactivity, and visual inspiration. These searches usually resulted in more graphic design and photography content - a helpful nuance to the more data-focused inspiration from the Tableau community.\r\nBy the end of this exercise, I had sketched out my new Tableau interactive resume layout and was ready to move on to designing the content:\r\nTableau resume version II layoutDashboard Background Design with Figma\r\nAt this stage in the development process, you can let your creativity and inspiration thrive. I have seen countless ways to design and customize the layout and format of an interactive resume. My preferred path to achieving a custom design and background is with an online tool I recently discovered thanks to the Twitter #DataFam community called Figma. Figma is a powerful online collaborative graphics editor with tons of useful features and helpful ways to systematically and accurately design a Tableau background within a space. I have been using Figma for over 6 months on projects including my interactive resume, at work, and in my dashboards for the weekly #MakeoverMonday Tableau community challenge.\r\nIf you are new to Figma, I would suggest researching some tutorials online to get started. I recommend watching this video by Lindsey Betzendahl [@ZenDollaData](https://twitter.com/ZenDollData) as a good starting point.\r\nFor my particular Tableau resume background, I relied on Figma to create the background shapes, color scheme, icons, and text boxes. Here’s a link if you would like to check out my design!\r\nBackground designed with FigmaData Prep\r\nNow that we have a background/shell to work with, it’s time I decided which visualizations to build for my dashboard, how the visualizations would function, and other unique aspects of an interactive dashboard like ‘Contact Me’ links, tooltips, and showcasing my portfolio. But before we get too far into the weeds on specific chart types and the iterative process of determining the best way to visualize the skills and experience from your resume, we need to talk about data prep (and data generation).\r\nThis section may raise some eyebrows but, at the end of the day, you are the judge of the skills you possess, so quantifying those skills becomes a very subjective exercise. Naturally, I would recommend being honest. The best way I found to do this was to imagine I was in an interview and was asked to justify why each skill received its respective score or expertise level.\r\n\r\n\r\n\r\nvia GIPHY\r\n\r\nFrom there, the process of generating your resume data is fairly straightforward. See the following table below as examples of where to start:\r\nExperience and education example\r\nStart Year\r\nEnd Year\r\nInstitution/Employer\r\nRole\r\nAchievement\r\n01/01/2016\r\n01/01/2018\r\nUniversity A\r\nStudent\r\nDegree A\r\n06/01/2018\r\n12/01/2018\r\nEmployer A\r\nIntern\r\n\r\n01/01/2019\r\ntoday()\r\nEmployer B\r\nAnalyst\r\n\r\nTechnical skill example\r\nSkill\r\nType\r\nScore (out of 10)\r\nTableau\r\nSoftware\r\n8\r\nSQL\r\nCoding language\r\n7\r\nR\r\nCoding language\r\n5\r\nFigma\r\nDesign website\r\n2\r\nProfessional/soft skill example\r\nSkill\r\nYears of experience\r\nScore (out of 10)\r\nPolicy Writing\r\n=year(today())-2018\r\n5\r\nVerbal Communication\r\n=year(today())-2016\r\n8\r\nLeadership\r\n=year(today())-2019\r\n4\r\nFrom here, you’ll have enough data points to begin visualizing your quantified resume data in Tableau. A few quick things I’ll point out before moving on:\r\nIt should go without saying these tables are rough sketches of the types of data you want to collect, you should explore other elements of your resume to gain further granularity in your skills, education, and experience\r\nI recommend taking advantage of Tableau’s Google Drive connection capabilities. This way your data will always remain *safe on the cloud.\r\nA second point on the Google Drive functionality, notice how I used a variation of the today() calculation to future-proof some data points. I highly recommend incorporating some aspect of this formula on your resume so that it continually updates as time passes. Whether or not you visualize this part of the data is up to you.\r\nBe sure to match up your interactive resume with your real resume. The last thing you would want is for a recruiter or job to get confused between the two documents!\r\nFor the visualization process, I relied heavily on the Tableau community and the resume gallery linked above. I spent hours experimenting with radar charts to showcase my technical skills, Gantt charts to visualize my work history and education, and switching between stacked bar charts and lollipop charts for my professional skills. At the end of the day, building these visualizations and experimenting with my resume data were extremely useful steps to arrive at my end product. Below are links to the main sources of inspiration I pulled from for my interactive resume:\r\nVarun Varma\r\nEric Balash\r\nDavid Kelly\r\nPriyanka Bulla\r\nBuild & Publish\r\nWith respect to building and publishing your dashboard, this blog post won’t go into any detail on the technical aspects of building out the vizzes in my particular interactive resume. If that’s something I receive a lot of interest or feedback on, I will gladly write that up. In the meantime, as you are exploring the Tableau Resume Gallery, make sure to bookmark or favorite the vizzes that speak to you. When you are ready, download the workbooks and dissect how the author created the viz for you to replicate and customize with your personal resume data that we walked through above! And be sure to include some of the more unique content on your interactive resume including ‘Contact Me’ buttons/symbols, achievements and certifications with respective links, and definitely interests & hobbies! The possibilities for displaying these very personal details to you are endless and make great call-outs to your creativity!\r\nWithout anymore jabbering from me, here is my Tableau interactive resume:\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nLink: https://tabsoft.co/3l8gOEn\r\nMarch 2021 Update:\r\nIn my quest to become more skilled in data analytics and business intelligence tools, I have started branching out from Tableau (and R) to learn Microsoft’s Power BI software/language. I recently received very useful advice on ways to incorporate Power BI into my data visualization skillset by replicating dashboards or products I have made with Tableau in Power BI. The first dashboard that came to mind was my interactive resume. As a result, I was able to, without much fuss, replicate my interactive resume in Power BI with a few minor changes and some workarounds. Here it is!\r\nPower BI Interactive Resume\r\n\r\n\r\n",
    "preview": "posts/2021-03-11-tableau-interactive-resume/powerbi.png",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {},
    "preview_width": 648,
    "preview_height": 1165
  },
  {
    "path": "posts/2020-11-13-google-takeout/",
    "title": "Google Takeout",
    "description": "Analyzing My Google Search and Map History with Google Takeout service",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2020-11-13",
    "categories": [
      "data wrangling",
      "data visualization",
      "R",
      "ggplot2",
      "interactive",
      "HTML",
      "API",
      "mapping"
    ],
    "contents": "\r\n\r\nContents\r\nSetup and data work\r\nData Exploratoration\r\nWordclouds\r\nGoogle Maps\r\n\r\nPublished on November 20th, 2020\r\n“Hey Google, show me my search history.”\r\nIf you are like me, you Google just about everything from weather to conversions from tbsp to cup, how to feed a sourdough starter, TSLA stock, where is the nearest bar, and the list goes on and on. So when I stumbled upon a blog post explaining how to explore your own Google search activity on https://towardsdatascience.com, written by Saúl Buentello, @cosmoduende, I knew I had to take a stab at analyzing my own search history.\r\nI won’t go through the steps to generate your own Google search history extract, Saúl does a fantasic job walking you step-by-step through Google’s Takeout service in his blog Here. That being said, make sure to poke around the complete Goole’s Takeout service. These days, tech companies collect troves of your data, but they are also making it more accessible. It’s important to understand what data they collect, how to restrict data collected on you, and how to utilize it for yourself!\r\nI will pick up from Saúl’s blog where he begins to load libraries and read the Google Takeout data. I already have some experience with text analysis from my YMH two-part blog series and I’ve picked up a lot of tricks from that project. Because of my prior experience, I will be making changes and, what I consider, improvements to Saul’s code below and adding more complex text analysis as well.\r\nSetup and data work\r\nAs always, we’ll make use of the {pacman} package to load, install, and update the libraries we’ll use for this analysis (I have also noted what each package is used for). I’m also going to try out some new visualization packages like {highcharter}.\r\n\r\n\r\npacman::p_load(\r\n  tidyverse,     ## data cleaning\r\n  lubridate,     ## for working with dates\r\n  ggplot2,       ## plots\r\n  plotly,        ## interactive plots\r\n  janitor,       ## text cleaning\r\n  dygraphs,      ## interactive JavaScript charting library\r\n  reactable,     ## interactive tables\r\n  wordcloud,     ## cloud of words\r\n  rvest,         ## for manipulating html and xml files\r\n  tm,            ## text mining\r\n  tidytext,      ## text mining\r\n  highcharter    ## html interactive charts\r\n)\r\n\r\n# install.packages(\"devtools\")\r\ndevtools::install_github(\"hadley/emo\")\r\n\r\ntheme_set(theme_minimal())\r\n\r\n\r\n\r\nWhen you download your Google Takeout data, you will notice it comes as an HTML file. Take a second to load that file in your browser just to explore the raw data from Google. In order to read and manipulate those files in R, we’ll make use of the {rvest} package. Along with my search history, I’ve also downloaded my Google Map activity. I probably use Google Maps as much as Google Search and it made sense for me to grab that activity. I’m hoping the added map data provides more insights into my search behaviors and I’m very interested in mapping this data but that will depend on if Google exports the data with spatial variables like latitude and longitude. I would be surprised if they did not allow exporting of that data.\r\n\r\n\r\n## read initial data\r\nmysearchfile <- read_html(\"~/GitHub/blog_website_2.0/blog_data/google_takeout/Takeout/My Activity/Search/MyActivity.html\", encoding = \"UTF-8\")\r\n\r\n\r\n\r\nWith the data loaded, I took a lot of cues from Saúl on manipulating the html structured search data.\r\n\r\n\r\n## Scraping search date and time\r\ndateSearched <- mysearchfile %>% \r\n  html_nodes(xpath = '//div[@class=\"mdl-grid\"]/div/div') %>%\r\n  ## I gather this pulls the section of the HTML document with the relevant search data\r\n  str_extract(pattern = \"(?<=<br>)(.*)(?<=PM|AM)\") %>%\r\n  ## From that section of HTML search data, we extra the pattern that matches our desired date\r\n  mdy_hms()                                                  \r\n  ## lubridate month/day/year hour/minute/second\r\n\r\n## Scraping search text\r\ntextSearch <- mysearchfile %>% \r\n  html_nodes(xpath = '//div[@class=\"mdl-grid\"]/div/div') %>% \r\n  ## looks like this xpath holds all the relevant \r\n  str_extract(pattern = '(?<=<a)(.*)(?=<\/a>)') %>%           \r\n  ## this extracts the google search url product of each google search\r\n  str_extract(pattern = '(?<=\\\">)(.*)')                      \r\n  ## this extracts just the search phrase after the url\r\n  ## so a normal good search looks like \"www.google.com/search?q=YOUR+SEARCH+HERE\"\r\n  ## and this pipe simply extracts the 'YOUR+SEARCH+HERE' text\r\n\r\n## Scraping search type\r\ntypeSearch <- mysearchfile %>% \r\n  html_nodes(xpath = '//div[@class=\"mdl-grid\"]/div/div') %>% \r\n  str_extract(pattern = '(?<=mdl-typography--body-1\\\">)(.*)(?=<a)') %>%\r\n  str_extract(pattern = '(\\\\w+)(?=\\\\s)')\r\n\r\nsearchedData <- tibble(timestamp = dateSearched,\r\n                       date = as_date(dateSearched),\r\n                       day = weekdays(dateSearched),\r\n                       month = month(dateSearched, label = TRUE),\r\n                       year = year(dateSearched),\r\n                       hour = hour(dateSearched),\r\n                       search = textSearch,\r\n                       type = typeSearch)\r\n\r\nsearchedData <- searchedData %>% \r\n  mutate(day = factor(day, levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))) %>%\r\n  na.omit %>% \r\n  clean_names()\r\n  ## from janitor\r\n\r\nglimpse(searchedData)\r\n\r\n\r\nRows: 95,000\r\nColumns: 8\r\n$ timestamp <dttm> 2020-11-07 19:34:48, 2020-11-07 19:34:17, 2020-11-07 19:34:~\r\n$ date      <date> 2020-11-07, 2020-11-07, 2020-11-07, 2020-11-07, 2020-11-07,~\r\n$ day       <fct> Saturday, Saturday, Saturday, Saturday, Saturday, Saturday, ~\r\n$ month     <ord> Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, ~\r\n$ year      <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, ~\r\n$ hour      <int> 19, 19, 19, 18, 17, 17, 13, 12, 12, 12, 12, 12, 12, 12, 12, ~\r\n$ search    <chr> \"https://www.businessinsider.com/number-one-observatory-circ~\r\n$ type      <chr> \"Visited\", \"Visited\", \"Searched\", \"Searched\", \"Searched\", \"S~\r\n\r\nAnd just like that, we have our Google search data organized in a nice little table of 8 columns and 95,000 rows. Safe to say I’ve made a few searches in my day. Imagine having to look in a dictionary or research a question that many times 👀.\r\n\r\n\r\n\r\nData Exploratoration\r\nStarting with something simple, let’s look at overall searches by year. I would imagine that my search history would grow linearly as technology and cell phones becomes more and more relevant. I am also expecting a spike in 2020 due to COVID-19 and being home more.\r\n\r\n\r\n## searches over time\r\nsearchedData %>% \r\n  ggplot(aes(year, fill = stat(count))) +\r\n  geom_bar(width = 0.7) +\r\n  scale_fill_gradient(low = \"yellow\", high = \"red\") +\r\n  labs(title = \"My Google Search History Over Time\",\r\n       x = \"Year\", \r\n       y = \"Count\"\r\n       ) +\r\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\r\n  scale_x_continuous(breaks = c(2008:2020))\r\n\r\n\r\n\r\n\r\nWell that is surprising. Turns out my search activity reached it’s peak in 2017 after a drop in searches after 2014 and has been steadily declining since the peak in 2017. I have a few hypotheses on why my search activity looks like this. Hopefully we’ll figure out if any of my ideas hold true once we dive further into the data.\r\n\r\n\r\n## searches by month\r\nsearchedData %>% \r\n  filter(year > 2012 & year < 2021) %>%\r\n  group_by(year, month) %>% \r\n  summarise(searches = n()) %>% \r\n  hchart(type = \"heatmap\", hcaes(x = year, y = month, value = searches), name = \"Searches\") %>% \r\n  hc_title(text = \"Search History Over Time\") %>% \r\n  hc_xAxis(title = list(text=\"Year / Month\")) %>% \r\n  hc_yAxis(title = FALSE, reversed = TRUE) %>% \r\n  hc_tooltip(borderColor = \"black\",\r\n             pointFormat = \"{point.searches}\") %>% \r\n  hc_colorAxis(minColor = \"yellow\",\r\n               maxColor = \"red\") %>% \r\n  hc_legend(align = \"center\", verticalAlign = \"top\", layout = \"horizontal\")\r\n\r\n\r\n\r\n\r\nHere’s my first shot at an interactive heatmap chart from the {highcharter} package and I have already uncovered an anomaly in my search history using this visual! Something very interesting took place in September of 2017. I searched 1,919 times in September, the highest number of searches in a single month. The next closest month was April, 2014! Now before we look into this specific month, let’s look at a few other descriptive statistics on time of day and day of week search behavior.\r\n\r\n\r\n## search by hour\r\nsearchedData %>% \r\n  ggplot(aes(hour, fill = stat(count))) +\r\n  scale_fill_gradient(low = \"yellow\", high = \"red\") +\r\n  geom_bar() +\r\n  labs(title = \"Search History by Hour of Day\",\r\n       x = \"Hour\", \r\n       y = \"Count\"\r\n       ) +\r\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\r\n  scale_x_continuous(breaks = c(0:23))\r\n\r\n\r\n\r\n\r\nFor anyone needing evidence of insomnia, this may be a good test! Turns out my search history is directly correlated with my sleep schedule, which probably happens to be a good thing for my health. Not too surprising, my search history lines up with a 9-5 schedule sprinkled with evening searching. Overall, my searching mainly peaks throughout the day and experiences a sharp decline during the late evening and early hours when I’m sleeping.\r\n\r\n\r\n## search by day\r\nsearchedData %>% \r\n  ggplot(aes(day, fill = stat(count))) +\r\n  scale_fill_gradient(low = \"yellow\", high = \"red\") +\r\n  geom_bar() +\r\n  labs(title = \"Search History by Day of Week\",\r\n       x = \"Day\", \r\n       y = \"Count\"\r\n       ) +\r\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n\r\n\r\n\r\n\r\nIt also turns out my search history follows a normal work week, too. I search the most on Monday and by the time the weekend comes around, I’m searching far less and hopefully enjoying time outside or with friends and family.\r\n\r\n\r\nsearchedData %>%\r\n  group_by(day, hour) %>% \r\n  summarise(searches = n()) %>%\r\n  mutate(day = factor(day,\r\n                      levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))) %>% \r\n  hchart(type = \"heatmap\", hcaes(x = hour, y = day, value = searches), name = \"Searches\") %>%\r\n  hc_title(text = \"Search History Day/Time Heatmap\") %>% \r\n  hc_xAxis(title = list(text=\"Hour\")) %>% \r\n  hc_yAxis(title = FALSE, reversed = TRUE) %>% \r\n  hc_tooltip(borderColor = \"black\",\r\n             pointFormat = \"Searches: {point.searches}\") %>% \r\n  hc_colorAxis(minColor = \"yellow\",\r\n               maxColor = \"red\") %>% \r\n  hc_legend(align = \"center\", verticalAlign = \"top\", layout = \"horizontal\")\r\n\r\n\r\n\r\n\r\nTying these temporal analyses together, my search trends are fairly consistent by day and time. It does appear, though, as the week begins to wind down, my Google searching experiences a reduction in activity. Safe to say I may be partaking in a few early summer Friday’s at work! 🤣\r\nWordclouds\r\nAlright let’s take a look at a few wordclouds next, starting with a cloud of my complete search tokens and then moving onto some more specific analysis based on the observations we made above. I will use the {tidytext} package to extract the words from all of my searches. {tidytext} is an incredibly powerful text mining package; I would suggest getting comfortable with it if you plan on doing similar analyses with text mining data such as twitter, transcripts or books.\r\n\r\n\r\n## text mine google search phrases\r\ntextData <- searchedData %>% \r\n  select(date, search) %>%\r\n  mutate(search = tolower(search)) %>%\r\n  unnest_tokens(\"word\", search) %>% \r\n  ## count(word, sort = TRUE)\r\n  ## ^ to see how rough the data is before text cleaining\r\n  anti_join(stop_words) %>% \r\n  filter(!(word %in% c(\"https\", \"http\", \"amp\")))\r\n\r\ntextData <- textData[-grep(\"\\\\b\\\\d+\\\\b\", textData$word),]\r\ntextData <- textData[-grep(\"(.*.)\\\\.com(.*.)\\\\S+\\\\s|[^[:alnum:]]\", textData$word),]\r\n\r\ntextDT <- textData %>%\r\n  count(word, sort = TRUE)\r\n\r\n## side by side wordclouds\r\npar(mfrow=c(1,2))\r\n\r\n## wordcloud for search terms\r\ntextCloud <- wordcloud(words = textDT$word, main = \"Title\", \r\n                       freq = textDT$n,\r\n                       min.freq = 1, max.words = 125,\r\n                       random.order = FALSE, colors = brewer.pal(10, \"Spectral\"))\r\n\r\n\r\n\r\n\r\n\r\n\r\ntextDTSept <- textData %>%\r\n  filter(year(date) == 2017) %>%\r\n  filter(month(date) == 09) %>% \r\n  count(word, sort = TRUE)\r\n\r\n\r\ntextCloud <- wordcloud(words = textDT$word, main = \"Title\", \r\n                       freq = textDT$n,\r\n                       min.freq = 1, max.words = 125,\r\n                       random.order = FALSE, colors = brewer.pal(10, \"Spectral\"))\r\n\r\n\r\n\r\n\r\nSo if you haven’t figured it out yet, wordclouds are simple to make in R (after the data cleaning with {tidytext} ) and they are really useful in text analysis, making key terms ‘pop’ while also allowing the reader to make connections between words. Take the example of the left wordcloud of all my Google searches since 2012. I live in Chicago so it’s almost expected that Chicago is my most searched term, mainly to keep my Google searches local. Next up, I am a huge Reddit user and it’s clear from my wordcloud I search Reddit a lot. Aside from scrolling and entertainment, I also use Reddit for more practical things like advice, suggestions, coding, and unbiased reviews. Before I purchase any product or item, I will usually search the product on Google, followed by “Reddit comments” to read reviews and thoughts from other people around the world. A few other interested observations: Tableau is my most searched analysis tool followed by QGIS; I think R didn’t make it in the wordcloud because its just a letter (rmarkdown did make it on the wordcloud on the bottom right). Finally, lots of cooking terms in the wordcloud including recipe, knife, eats (serious eats), pizza, cheese, etc. I am an avid home chef so that’s no surprise to me (or my fiance!).\r\nNow, ironically, the wordcloud on the right, specific to the September 2017 spike we found, does not really offer any insights into why there were so many searches. That’s unfortunate but I have a few other ideas to try and pull out some more insights into this particular time period.\r\n\r\n\r\n## plot search terms over time, by term\r\nggplotly(\r\n  textData %>%\r\n    mutate(date = year(date)) %>% \r\n    mutate(date = str_sub(date)) %>% \r\n    group_by(date, word) %>%\r\n    summarise(count = n()) %>%\r\n    slice_max(order_by = count,\r\n              n = 10,\r\n              with_ties = TRUE) %>% \r\n    ggplot(aes(x = date, y = count)) +\r\n    geom_text(aes(size = count, label = paste(word), text = paste(\r\n      \"In\", date, \"I searched\", word, count, \"times\"))) +\r\n    labs(\r\n      title = \"Search Words By Year\",\r\n      x = \"Year\",\r\n      y = \"Search Count\"\r\n    ) +\r\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n, tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nAha! We’re finally getting somewhere on my 2017 Google search spike. Chicago was by far my most searched term in 2017, nearly double any other term I’ve ever searched on Google (which also turn out to be ‘Chicago’, ironically). This plot also shows my progression (and linear growth) of search terms over time - definitely a trip down memory lane! Back in 2008, my top search was for ‘blackberry’. In 2009, ‘windows’ was my top search because Windows 7 was released that year and I distinctly remember participating in the Windows beta/developer versions which, by the way, was a terrible idea to test out an unstable version of Windows while simultaneously attending college. Between 2010 and 2016, my search history was noisy and difficult to interpret until I decided to resign as a police officer, move to Chicago, and attend grad school at the University of Chicago around the summer of 2016 when Chicago began to leap into my top search term. After 2016, my search history clears up to the themes we identified in the wordclouds. This plot was so helpful in digging into my search history, I wonder what a plot of key search terms specific to 2017 would look like …\r\n\r\n\r\n## plot search terms for 2017\r\nggplotly(\r\n  textData %>%\r\n    filter(year(date) == 2017) %>%  \r\n    group_by(date, word) %>%\r\n    mutate(date = factor(months(date))) %>% \r\n    summarise(count = n()) %>%\r\n    slice_max(order_by = count,\r\n              n = 10,\r\n              with_ties = TRUE) %>% \r\n    ggplot(aes(x = date, y = count)) +\r\n    geom_text(aes(size = count, label = paste(word), text = paste(\r\n      \"In\", date, \"I searched\", word, count, \"times\"))) +\r\n    labs(\r\n      title = \"Search Words by Month, 2017\",\r\n      x = NULL,\r\n      y = \"Search Count\"\r\n    ) +\r\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())\r\n, tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\n\r\n\r\n Wonder no further! Chicago was consistently one of my most searched terms in 2017, clearing up the reason why it was the top search for the year and why 2017 was such an outlier. Thinking back, the year 2017 brought a lot of changes and challenges to my life. I completed my graduate thesis and graduated from UChicago with my second master’s, began a long and arduous job hunt, and started a new job at the UChicago Crime Lab. With that in mind, I’m not too surprised this year had such a high search volume.\r\nImportant mental note here - with my start at a new job, my Google activity was essentially split between my personal Google account and my work Google account. Thus, the drop off in activity and starting in 2017 - though I’m not sure why it has not leveled off yet.\r\nGoogle Maps\r\n\r\n\r\npacman::p_load(jsonlite,  ## reading json files\r\n               ggmap)     ## statis mapping\r\n\r\npacman::p_load_gh(c(\"r-spatial/mapview\",\r\n                   \"dkahle/ggmap\"))\r\n## install mapview from github, there is a bug when knitr'ing to HTML with the CRAN version\r\n\r\n\r\n\r\n\r\n\r\n## Not ran - time intensive process to load, manipulate and write 2.2gb json file of location history.\r\n## I suggest running this once with the cleaning steps and writing to another file, as I have done below\r\n\r\n# # load google map location data\r\n# loc_hist_takeout <- fromJSON(\"~/GitHub/kjmagnan1s.github.io/blog_data/google_takeout/Takeout/My Activity/Maps/Location History.json\", flatten = TRUE)\r\n# locations <- loc_hist_takeout$locations\r\n# rm(loc_hist_takeout) ## free up memory from large json files\r\n# \r\n# ## clean json data\r\n# locations <- locations %>% \r\n#   ## convert the time column from milliseconds\r\n#   mutate(time = as.POSIXct(as.numeric(timestampMs)/1000, origin = \"1970-01-01\")) %>%\r\n#   ## clean/arrange date columns\r\n#   mutate(day = day(time)) %>% \r\n#   mutate(day = ordered(day, levels = c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"))) %>% \r\n#   ## conver lat/long from E7 to coordinates\r\n#   mutate(lat = latitudeE7/1e7) %>% \r\n#   mutate(long = longitudeE7/1e7) %>% \r\n#   ## remove unnecessary columns\r\n#   select(time, lat, long)\r\n# \r\n# ## write to csv for later use, maybe a Tableau dashboard\r\n# write.csv(locations, file = \"~/GitHub/kjmagnan1s.github.io/blog_data/google_takeout_location_data.csv\")\r\n\r\n\r\n\r\n\r\n\r\n## load pre-cleaned location history data\r\nlocations <- read.csv(\"~/GitHub/blog_website_2.0/blog_data/google_takeout_location_data.csv\")\r\nglimpse(locations)\r\n\r\n\r\nRows: 1,760,771\r\nColumns: 4\r\n$ X    <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19~\r\n$ time <chr> \"2012-10-26 11:23:55\", \"2012-10-26 11:24:57\", \"2012-10-26 11:25:5~\r\n$ lat  <dbl> 37.30934, 37.30861, 37.30931, 37.30861, 37.30931, 37.30861, 37.30~\r\n$ long <dbl> -89.53574, -89.53561, -89.53569, -89.53561, -89.53562, -89.53561,~\r\n\r\n\r\n\r\n# ## load map searches\r\nmymapfile <- read_html(\"~/GitHub/blog_website_2.0/blog_data/google_takeout/Takeout/My Activity/Maps/MyActivity.html\", encoding = \"UTF-8\")\r\n\r\n## re-running the cleaning code on the google map searches\r\n## Scraping map date and time\r\ndateMapped <- mymapfile %>% \r\n  html_nodes(xpath = '//div[@class=\"mdl-grid\"]/div/div') %>%\r\n  str_extract(pattern = \"(?<=<br>)(.*)(?<=PM|AM)\") %>%\r\n  mdy_hms()                                         \r\n\r\n\r\n## Scraping map text\r\ntextMapped <- mymapfile %>% \r\n  html_nodes(xpath = '//div[@class=\"mdl-grid\"]/div/div') %>% \r\n  str_extract(pattern = '(?<=<a)(.*)(?=<\/a>)') %>%           \r\n  str_extract(pattern = '(?<=\\\">)(.*)')                      \r\n\r\n## Scraping map type\r\ntypeMapped <- mymapfile %>% \r\n  html_nodes(xpath = '//div[@class=\"mdl-grid\"]/div/div') %>% \r\n  str_extract(pattern = '(?<=mdl-typography--body-1\\\">)(.*)(?=<a)') %>%\r\n  str_extract(pattern = '(\\\\w+)(?=\\\\s)')\r\n\r\nmappedData <- tibble(timestamp = dateMapped,\r\n                       date = as_date(dateMapped),\r\n                       day = weekdays(dateMapped),\r\n                       month = month(dateMapped, label = TRUE),\r\n                       year = year(dateMapped),\r\n                       hour = hour(dateMapped),\r\n                       search = textMapped,\r\n                       type = typeMapped)\r\n\r\nmappedData <- mappedData %>% \r\n  mutate(day = factor(day, levels = c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))) %>%\r\n  #na.omit %>% \r\n  clean_names()     \r\n  ## from janitor\r\n\r\nglimpse(mappedData)\r\n\r\n\r\nRows: 58,226\r\nColumns: 8\r\n$ timestamp <dttm> 2020-11-12 16:28:10, 2020-11-12 16:28:03, 2020-11-12 16:27:~\r\n$ date      <date> 2020-11-12, 2020-11-12, 2020-11-12, 2020-11-12, 2020-11-12,~\r\n$ day       <fct> Thursday, Thursday, Thursday, Thursday, Thursday, Thursday, ~\r\n$ month     <ord> Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, Nov, ~\r\n$ year      <dbl> 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, 2020, ~\r\n$ hour      <int> 16, 16, 16, 10, 10, 10, 9, 9, 17, 17, 17, 17, 17, 17, 15, 15~\r\n$ search    <chr> NA, \"Viewed area in Google Maps\", \"Lonesome Rose\", NA, \"Beve~\r\n$ type      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Sea~\r\n\r\nIf there is any other tech service I use as much as Google Search, it would be Google Maps. I thought it would be fun to compare my Google Maps activity now that I have analyzed my search activity. I’m expecting much less Google Map activity throughout the week and significant increases in activity at night and on the weekends. As we saw before, my Google activity could very easily surprise me, although I expect less surprises with my map data as I assume most of my activity is from my phone and during personal time. I have also exported my Google location history data and I’m super excited to play around with mapping that data. I have a lot of experience in GIS and spatial analysis with Tableau, QGIS, ArcGIS, and GeoDa but not very much in R so I’m looking forward to experimenting with that next.\r\n\r\n\r\n## map searches over time\r\nmappedData %>% \r\n  ggplot(aes(year, fill = stat(count))) +\r\n  geom_bar(width = 0.7) +\r\n  scale_fill_gradient(low = \"yellow\", high = \"red\") +\r\n  labs(title = \"My Google Search History Over Time\",\r\n       x = \"Year\", \r\n       y = \"Count\"\r\n       ) +\r\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\r\n  scale_x_continuous(breaks = c(2008:2020))\r\n\r\n\r\n\r\n\r\nSee now this is the kind of chart I expected for my Google Searches! What an interesting ramp up of activity over the years, especially between 2016 and 2017 - again around the time I moved to Chicago and started exploring the city. And we finally have a good indication of how COVID has impacted my search behaviors. I have been socially distant and at home for most of the year. That really shows with the dramatic drop in Google map search activity in 2020. This was the full list of activity, including directions, viewed areas (which I learned Google tracks), and some ‘NA’ values. Let’s remove those, add some more granularity, and see if that changes this plot.\r\n\r\n\r\n## map views/searches by year\r\nggplotly(\r\n  mappedData %>% \r\n  mutate(type = (ifelse(grepl(\"Viewed area\", search), \"Viewed\", \"Searched\"))) %>% \r\n  ## notate rows which are Google Map views or searches in the app\r\n  na.omit() %>% \r\n    group_by(year, type) %>% \r\n    summarise(count = n()) %>% \r\n  ggplot(aes(year, count, fill = type, text = paste(\r\n    \"Year:\", year,\r\n    \"<br>Count:\", count))) +\r\n  geom_col(width = 0.7) +\r\n  labs(title = \"My Google Search History Over Time by Type\",\r\n       x = \"Year\", \r\n       y = \"Count\"\r\n       ) +\r\n    scale_fill_manual(breaks = c(\"Searched\", \"Viewed\"),\r\n                      values = c(\"red\", \"gold\")) +\r\n    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +\r\n  scale_x_continuous(breaks = c(2008:2020)\r\n                     ), tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nFascinating! This data appears to be a symptom of how Google has recorded map activity overtime, but it would seem that my map viewing on Google Maps (navigating through Google Maps via the map and not searching addresses or locations) has increased dramatically over time while my Google Map searches (searching addresses or specific locations) have remained relatively stable since 2016. The dramatic increase in viewed map activity in 2017 is so dramatic that I think Google did actually change the way they record activity around this time. However, there is an argument to be made that my viewed map activity increased this much due to increased accessibility, speed of mobile networks, and moving to a new city. Nonetheless, I’ll take these insights; my viewed map activity has increased nearly 3x since 2017.\r\nLet’s take a loot at a few more heatmaps, followed by some real maps and we’ll call this a day!\r\n\r\n\r\n## map activity over time, heatmap\r\nmappedData %>%\r\n  ## filter out data before 2011, was incomplete\r\n  filter(year > 2011) %>% \r\n  na.omit() %>%\r\n  group_by(year, month) %>% \r\n  summarise(count = n()) %>% \r\n  hchart(type = \"heatmap\", hcaes(x = year, y = month, value = count), name = \"Searches\") %>%\r\n  hc_title(text = \"Map History Year/Month Heatmap\") %>% \r\n  hc_xAxis(title = list(text=\"Year\")) %>% \r\n  hc_yAxis(title = FALSE, reversed = TRUE) %>% \r\n  hc_tooltip(borderColor = \"black\",\r\n             pointFormat = \"In {point.month} of {point.year}, I searched {point.count} times\") %>% \r\n  hc_colorAxis(minColor = \"yellow\",\r\n               maxColor = \"red\") %>% \r\n  hc_legend(align = \"center\", verticalAlign = \"top\", layout = \"horizontal\")\r\n\r\n\r\n\r\n\r\nWhat I really like about these heatmaps are that they show a clear progression through time but also provide a lot of granularity compared with a simple bar chart and there is not a need to do any math in your head to visualize changes over time. For example, there is a clear growth of Google Map searches through 2018, and it looks like it really picks up in 2016. However, we can see a substantial and isolated spike in activity in February 2014, which did not happen again until my activity started to increase more in 2016. We also see two months in 2018 which impacted the years overall activity - August and December. These months were both over 200 searches and no other month has peaked over 200 in all of my activity. The COVID impact is also extremely apparent in the 2020 column. It closely resembles my activity as far back as 2014/15.\r\n\r\n\r\n## map activity over time, heatmap\r\nmappedData %>%\r\n  na.omit() %>% \r\n  group_by(day, hour) %>% \r\n  summarise(count = n()) %>% \r\n  hchart(type = \"heatmap\", hcaes(x = hour, y = day, value = count), name = \"Searches\") %>%\r\n  hc_title(text = \"Map History Day/Time Heatmap\") %>% \r\n  hc_xAxis(title = list(text=\"Hour\")) %>% \r\n  hc_yAxis(title = FALSE, reversed = TRUE) %>% \r\n  hc_tooltip(borderColor = \"black\",\r\n             pointFormat = \"At {point.hour}:00 hours, I searched {point.count} times\") %>% \r\n  hc_colorAxis(minColor = \"yellow\",\r\n               maxColor = \"red\") %>% \r\n  hc_legend(align = \"center\", verticalAlign = \"top\", layout = \"horizontal\")\r\n\r\n\r\n\r\n\r\nHere’s another sanity check for insomnia… Turns out I don’t use Google Maps very much after around midnight until 7 am which is a good sign! My activity throughout the day is spread out and is not too surprising. Quick note, we see my summer Friday activity creeping up again at 11:00 hours where I’m most likely looking for a place to go grab a drink and some food with coworkers.\r\n\r\n\r\n## map my location history data for Chicago\r\nregister_google(Sys.getenv(\"my_api\"), ## Your API key goes here!\r\n                account_type = \"standard\")\r\n\r\nchi_town <- get_map(\"Chicago, Illinois\", zoom = 11)\r\n\r\nggmap(chi_town) +\r\n  stat_density2d(data = locations, aes(x = long, y = lat, fill = ..level.., alpha = ..level..),\r\n             size = 1, bins = 1000,\r\n             geom = \"polygon\") +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        axis.title = element_blank(),\r\n        axis.text.y = element_blank(),\r\n        )\r\n\r\n\r\n\r\n\r\nFinally, some mapping! This map is terrific and terrifying at the same time. It is 100% accurate in that it shows where I spend most of my time in Chicago, which is the terrifying part because Google knows exactly where I live, work, and went to school (which honestly should not be that surprising). The two intense clusters north of downtown, in Uptown and Bucktown, show where I’ve lived in Chicago. The most south cluster represents my time at UChicago and the middle three clusters represent the locations I mostly work out of. I actually found it really interesting to focus on the opaque, light gray clustered activity which closely follows the main roads I take to go grocery shopping or take the ‘L’ to downtown.\r\n\r\n\r\n# map my specific location for December 2018\r\nlocations %>%\r\n  filter(year(time) == 2018) %>%\r\n  filter(month(time) == 12) %>%\r\n  mapview(xcol = \"long\", ycol = \"lat\", crs = 4326,\r\n          homebutton = mapviewGetOption(\"homebutton\"), grid = FALSE, label = \"time\", layer.name = \"Home\", legend = FALSE)\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-11-13-google-takeout/google_history.png",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-10-28-wow-buzz-analysis/",
    "title": "WoW Buzz Analysis",
    "description": "Analysis of My World of Warcraft Classic Guild Loot System Data",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2020-10-28",
    "categories": [
      "data wrangling",
      "data science",
      "data visualization",
      "R",
      "ggplot2",
      "interactive",
      "tableau",
      "gaming"
    ],
    "contents": "\r\n\r\nContents\r\nSetup and data work\r\nDescriptives\r\nDecay\r\nEffort Points and Gear Points\r\nRollin’ for Loot \r\nTableau Public Interactive Infographic\r\nClosing Up\r\n\r\nHaving recently started blogging, I realized during my two part YMH post that writing and analysis become much easier when they are topics I enjoy (surprise, surprise right?!). Even more, learning and refining my analytical skills is very fulfilling when combined with topics that I find entertaining. With that in mind, I thought what better data to work with next than World of Warcraft Classic. This is a game I spent much of my childhood playing and I have many, many fond memories of this game. As you can imagine, when Blizzard Entertainment announced Classic Wow, the nostalgic teenager in me could not resist.\r\nNow before the the nerd alarms start going off, let me explain…\r\n\r\n\r\n\r\n… World of Warcraft Classic (‘Classic WoW’) is Blizzard Entertainment’s recreation of the 2004 critically acclaimed and world’s most popular MMORPG game, commonly referred to as World of Warcraft Vanilla1. Part of the allure and success of Classic WoW can be directly attributed to its emphasis of social gameplay. In order to complete end-game content, players must assemble and coordinate massive 25 to 40 person groups through large dungeons or zones, called Raids, within the game, overcoming game mechanics, role requirements and duties, and leadership challenges, all with the reward of powerful end-game epic items, referred to as “loot”. In order to be able to coordinate with 40 other people, in-game organizations, called Guilds, were created for players to socialize and more easily organize towards end-game objectives.\r\nEven though a lot of things have changed since 2004, the need to find and join a good guild is as important as ever. What I found after joining my guild, Buzz, is that the playerbase of classic WoW is essentially the same as 2004, but 15 years older. Thus, most guilds are comprised of mid-career professionals with children, lifestyles, and far more life experiences who played WoW back in 2004.\r\nThis story of this idea begins a few months ago when our guild began running into some issues with guild management and loot distribution. See, loot in Classic WoW is extremely sought-after, so much so that there are countless stories of guilds completely disbanding over arguments about loot - a ‘Google’ search of “WoW loot drama” returns 1.9 million results. This led to guilds instituting loot rules or loot systems, such as DKP (Dragon Kill Points), Loot Council, or EPGP (Effort Points/Gear Points). Our guild, for example, decided early on in its formation that EPGP would be our method of loot distribution (I’ll cover the elements of EPGP later in this post). When we began end-game content, we utilized a free-to-use EPGP Discord bot. But as of late, this bot has become unreliable and not easily customizable. It was at this point that one of our members skilled in C# coding decided to write a bot personalized to meet Buzz’s needs.\r\nNow that a few months have elapsed since we deployed our own custom bot, I decided this was a great time to utilize a completely tracked EPGP loot system using R and Tableau. My hope is that by looking into this data, some of our guild members will spend time learning about how useful our bot is but also learn a little more about our guild and loot behavior.\r\nBefore we proceed, there are probably a few key terms and gaming concepts I should mention for anyone new to this universe:\r\nRaid zones in Classic WoW allow for up to either 25 players or up to 40 players, depending on the raid. Raids are the main source of EPGP and loot in Classic WoW\r\nEPGP is an accounting and tracking system for loot distribution. Participation in raids and other shared guild objectives accrue Effort Points (EP). Acquiring loot or in-game items increases Gear Points (GP). The ratio of Effort Points to Gear Points determine members priority (PR) for loot. The highest players PR determines who receives the loot: 2\r\n\r\n\\(PR = \\frac{EP}{GP}\\)\r\n\r\n\r\nThe EPGP Bot tracks, among other things, players attendance to Raids, contributions to guild objectives, items awarded to players, and their EPGP history of accruing points. The Buzz bot functions by interacting with Discords REST API services and user input.\r\nEffort Points, or EP, are pre-determined amounts for Raid participation. For guild objectives and items, average market in-game cost for these items is converted in EP.\r\nGear Points, or GP, are pre-determined amounts for every item in Classic WoW, primarily based on the items attributes. 2 The exact GP equation for Buzz relies on item value and an items slot value: 2\r\n\r\n\\[GP = IV^2 * 0.04 * SV\\]\r\n\r\n\r\n\\[\r\nIV = \r\n\\begin{cases}\r\n\\frac{(I_lvl - 4)}{2}  if Q \\leq 2\r\n\\\\\r\n\\frac{I_lvl - 1.84}{1.6}  if Q \\ = 3 \r\n\\\\\r\n\\frac{I_lvl-1.3}{1.3}  if Q \\geq 4\r\n\\end{cases}\\]\r\n\r\n\r\nOur guild primarily raids on Sunday and Tuesday nights after the Classic WoW US servers reset and EP points are awarded for guild objective and item donations on Sundays. As a result, any temporal analysis would not produce particularly insightful findings.\r\nThe EPGP bot has gone through multiple iterations, with new changes and features added over time. This may result in slight inconsistencies across the data, though, the core elements of the bot have remained constant.\r\nThe EPGP bot has multiple data exporting capabilities. The goal of this blog post is to explore the EPGP data and learn some insights about Buzz.\r\n\r\nSetup and data work\r\nThere isn’t much work to do here, the EPGP bot’s data is fairly clean outside of a chunk of rows that do not have the correct date assigned to their entries.\r\n\r\n\r\npacman::p_load(            ## load libraries at once, this package is a game changer\r\n               tidyverse,\r\n               lubridate, \r\n               ggraph,     ## ggplot\r\n               vroom,      ## improved csv reading and in-line piping during load\r\n               reactable,  ## interactive tables\r\n               plotly)     ## interactive plots\r\n\r\n## epgp <- vroom(\"~/GitHub/kjmagnan1s.github.io/blog_data/buzz_epgp_10.16.20.csv\")\r\n## loot <- vroom(\"~/GitHub/kjmagnan1s.github.io/blog_data/buzz_loot_10.16.20.csv\")\r\n\r\nt_actions <- vroom(\"~/GitHub/kjmagnan1s.github.io/blog_data/buzz_transactions_10.21.20.csv\") %>% \r\n  ## removing time stamps from date variable\r\n  mutate(TransactionDate = as.Date(TransactionDateTime, format = '%m/%d/%Y')) %>% \r\n  ## reassigned all incorrectly entered dates to August 1st, it was around this time when these records were assigned this date incorrectly and it was resolved.\r\n  mutate(TransactionDate = replace(TransactionDate, TransactionDate == \"0001-01-01\", \"2020-08-01\")) %>% \r\n  mutate(Memo = str_remove_all(Memo, c(\"\\\\]|\\\\[\"))) %>%\r\n  mutate(Type = ifelse(grepl(\"Roll\", Memo), \"Roll\", \"Claim\")) %>%\r\n  mutate(Memo = str_remove_all(Memo, c(\"Claim|Roll\"))) %>% \r\n  mutate(Memo = tolower(Memo))\r\n  \r\n  \r\nwrite.csv(t_actions, file = \"~/GitHub/kjmagnan1s.github.io/blog_data/buzz_transactions_clean_10.21.20.csv\")\r\n\r\nglimpse(t_actions)\r\n\r\n\r\nRows: 16,020\r\nColumns: 9\r\n$ Id                  <chr> \"60646a79-4939-4326-8c97-0c7f5adf7165\", \"23a3db...\r\n$ TransactionDateTime <chr> \"10/21/2020 2:51:16 AM\", \"10/21/2020 2:51:16 AM...\r\n$ DiscordUserId       <dbl> 1.217641e+17, 1.884660e+17, 1.887478e+17, 7.167...\r\n$ CharacterName       <chr> \"Cantuna\", \"Ganzu\", \"Azar\", \"Jaydawg\", \"Lindo\",...\r\n$ TransactionType     <chr> \"EpAutomated\", \"EpAutomated\", \"EpAutomated\", \"E...\r\n$ Value               <dbl> 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,...\r\n$ Memo                <chr> \"raid end bonus\", \"raid end bonus\", \"raid end b...\r\n$ TransactionDate     <date> 2020-10-21, 2020-10-21, 2020-10-21, 2020-10-21...\r\n$ Type                <chr> \"Claim\", \"Claim\", \"Claim\", \"Claim\", \"Claim\", \"C...\r\n\r\nAfter chatting with our Buzz bot developer, he made a new export function for me to get a much more robust dataset than the ones he programmed for the bot to export (‘epgp’ and ‘loot’ commented out above). With that new export, we have 8 columns and just over 16,000 rows of data. Let’s explain the variables and data below:\r\nDiscordUserID is the ID associated with a user. Because players can play more than one ‘character’, the bot tracks each of those characters EPGP separately but Discord still tracks unique user ID’s. This variable will allow us to track user activity across all their characters.\r\nCharacterName is the in-game character name\r\nTransactionType describes the EPGP value entry type based on in-game actions. The types are start with either a EP (+) or GP (-) classification followed by the following:\r\nEpAutomated: EP awards for participating in raid, automatically assigned based on raid characteristics\r\nEpDecay: Decay is used as an incentive to use the EPGP system and not hoard PR\r\nEpManual: Usually for EP adjustments or awards for in-game guild contributions\r\nGpDecay: Decay is used as an incentive to use the EPGP system and not hoard PR\r\nGpFromGear: GP awards for claiming loot/gear with PR\r\nGpManual: Usually for EP adjustments/corrections\r\n\r\nValue is the amount of EP or GP for the transaction\r\nMemo is a short detailed description of the transaction type. Majority of these are automated but manual corrections would be entered here as well\r\nTransactionDateTime and TransactionDate (added my me) are the dates and times of the entries\r\nType (added by me) is arbitrary and used for one of the charts. I’ll explain more later.\r\nWith the data cleaning and explanations out of the way, lets explore the EPGP data with some descriptive analyses in {r} before moving into an interactive Tableau Public dashboard.\r\nDescriptives\r\nStarting with something simple, I wanted to see the most expensive items for GP that were claimed since the start of our new bot:\r\n\r\n\r\nt_actions %>% \r\n  filter(TransactionType == \"GpFromGear\") %>%\r\n  group_by(CharacterName, Memo, TransactionDate) %>%\r\n  summarise(Value) %>%\r\n  filter(Value > 15) %>%                              ## avoids any of the Rolls, we'll cover those later\r\n  reactable(defaultSorted = list(\"Value\" = \"desc\"))   ## interactive table\r\n\r\n\r\n\r\n\r\nSome interesting findings in this table. I was really looking to see which character/player claimed the highest GP valued loot. Turns out it was a tie at 301 GP: Phanon for Ashkandiand Banesoul for Staff of the Shadow Flame- which, ironically, both drop from Nefarian, the final boss in Blackwing Lair (BWL). I also found it interesting that most of the items claimed at the top of the list occurred very recently, within the past 2 months, even though about half of the top 10 items in this list are from BWL, which we have been raiding for months. I guess we’ve had some great loot recently!\r\nThis is probably a good time to mention that the character I play is Okara (don’t ask, was the name I randomly chose back in Vanilla and I felt I had to keep it in Classic), seen on this list near the top for claiming the Blessed Qiraji Augur Staff for 286 GP.\r\nLets take this table view and make a plot of the top 20 items claimed, just to look at it visually instead:\r\n\r\n\r\nggplotly(\r\n  t_actions %>%\r\n  filter(TransactionType == \"GpFromGear\") %>%\r\n  distinct(TransactionType, Memo, Value) %>%\r\n  slice_max(order_by = Value, n = 20) %>%\r\n  mutate(Memo = str_remove_all(Memo, \"Claim\")) %>%\r\n  ggplot(aes(x = fct_reorder(Memo, Value, .desc = TRUE), y = Value, text = paste(\r\n    \"<br> Item:\", Memo,\r\n    \"<br> GP Amount:\",Value))) +\r\n  geom_col(position = \"dodge\") +\r\n  theme(axis.text.x = element_text(angle = 45)) +\r\n    labs(\r\n    title = \"Top Loot for GP\",\r\n    x = NULL,\r\n    y = \"GP Amount\"\r\n  ), tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nRight so nothing really jumps out at me here. The GP values are ever so slightly skewed to the right, indicating a smooth reduction in GP cost as the items get cheaper. One important note that any WoW player would catch onto, the top 20 list here are all weapons. In the EPGP system, weapons are normally the most sought after item and come with the highest GP values.\r\nThe next descriptive type of analysis I wanted to see was overall EP and GP accruals since July. Without getting too technical, I’m interested in finding out how the distribution of EP and GP looks across the guild and if there are any outliers I can detect.\r\n\r\n\r\nggplotly(\r\n  t_actions %>%\r\n  select(CharacterName, TransactionType, Type, Memo, Value) %>%\r\n  filter(!(Memo == \"20% decay\")) %>%\r\n  group_by(CharacterName) %>% \r\n  summarise(EP = sum(Value[TransactionType %in% c(\"EpAutomated\", \"EpManual\")]),\r\n            GP = sum(Value[TransactionType %in% c(\"GpFromGear\", \"GpManual\")])) %>%\r\n  gather(EPGP, Value, -CharacterName) %>% \r\n  filter(Value > 10) %>%\r\n  slice_max(order_by = Value, n = 100) %>%\r\n  ggplot(aes(x = fct_reorder(CharacterName, Value, .desc = TRUE), y = Value, fill = EPGP, text = paste(\r\n    CharacterName, \"has accrued\", Value, EPGP, \"points\"))) +\r\n  geom_col(position = \"dodge\") +\r\n  theme(axis.text.x = element_text(angle = 75)) +\r\n    labs(\r\n    title = \"Top 100 EPGP Values\",\r\n    x = \"Character\",\r\n    y = \"EPGP Amount\"\r\n  )\r\n  ,tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nOkay so there are quite a few things to point out in this chart. First, in my code I filtered out the ‘decay’ values because I wanted to look at overall EP and GP values and the decay values are arbitrary to promote using the system. Second, we have a lot of guild members in the system and a number of ‘alts’, or secondary characters to a Discord users main character, that accrue EPGP. I removed a portion of characters with very low EPGP values in my code (>10) but there are still clearly a lot of characters with EPGP values. Lastly, a reminder that these charts are interactive, you can zoom, pan, and compare data with your mouse and using the tooltips.\r\nNow with that out of the way, a number of values strike me as intriguing. Baxterwar, one of our guild’s main tanks, has earned one of the highest EPGP values in the guild but also spent the most GP in the guild (mostly on gear to benefit the guild because tanks are so crucial. Thanks, Baxter!) on items like Thunderfury. Damshaman, one of our main character’s alt, has claimed a lot of epic items from raids in a very short time so his EPGP is one of the only negative values I found! Finally, the juxtaposition of Triqueta and HighPriority’s EP and GP is incredibly unique among players. This equates to these players having some of the highest PR in the guild!\r\nDecay\r\nDecay values were, at first, a bit tricky to think about. After talking with some guild members, I have a few ideas on how to view EPGP decay now. My guess is that because decay is out of sight and out of mind after having been announced, it has not changed behavior very much. I think that might change after players see this.\r\nNote: decay is applied flat, at fixed intervals - 20% weekly - to discourage EP hoarding.\r\n\r\n\r\nggplotly(\r\n  t_actions %>% \r\n  select(CharacterName, TransactionType, Type, Memo, Value, TransactionDate) %>%\r\n  group_by(CharacterName) %>% \r\n  mutate(Value = abs(Value)) %>%\r\n  summarise(Max_Raid_Date = max(TransactionDate[TransactionType %in% \"EpAutomated\"]),\r\n            EP_Decay = sum(Value[TransactionType %in% c(\"EpDecay\")]),\r\n            EP_Value = sum(Value[TransactionType %in% c(\"EpAutomated\", \"EpManual\")])) %>%\r\n  mutate(Ratio = (EP_Decay/EP_Value)) %>% \r\n  filter(Ratio > 0) %>% \r\n  slice_max(order_by = Ratio, n = 75) %>%\r\n  ggplot(aes(x = fct_reorder(CharacterName, Ratio, .desc = TRUE), y = (Ratio), text = paste(\r\n    CharacterName, \"has lost\", EP_Decay, \"EP to Decay. Put differently,\", scales::percent(Ratio, accuracy = 0.1), \"of their EP has Decayed\",\r\n    \"<br>Their last raid date was:\", Max_Raid_Date))) +\r\n  geom_col(position = \"dodge\") +\r\n  theme(axis.text.x = element_text(angle = 75)) +\r\n    labs(\r\n    title = \"Top 50 Decay Values as a % of EP Earned\",\r\n    x = \"Character\",\r\n    y = \"EP Decay/EP Earned\"\r\n  ) + scale_y_continuous(labels = scales::percent),\r\n  tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nInitially, I began looking at decay across the board and was not very interested with the findings. I quickly realized that raw decay values are nothing more than a representation of a players behavior and engagement in the game. Since it is a flat 20% decay, it mimics overall EPGP values and does not add much to the conversation. However, if we look at decay as a ratio of player behavior metrics, it becomes much more interesting.\r\nFor example, the first 10 or so players in this chart have lost almost 100% of the EP they earned through raiding and guild contributions. Many of these players have since quit the guild or have not participated in weekly raids for a few months. As we move down the x-axis, we start coming across more veteran guild members who have obtained most of their ‘best-in-slot’ gear and no longer need loot but continue to raid to help others obtain loot.\r\nNote: These values do not include the commensurate GP decay which is applied at the same amount (20%) and interval (weekly). Therefore, these values do not provide a complete picture of a character’s PR.\r\n\r\n\r\nitems_by_char <-t_actions %>% \r\n  select(CharacterName, TransactionType, Value) %>%\r\n  filter(TransactionType == \"GpFromGear\") %>% \r\n  count(CharacterName, name = \"Items_Claimed_GP\", sort=TRUE)\r\n\r\nreactable(items_by_char)\r\n\r\n\r\n\r\n\r\nggplotly(\r\n  t_actions %>% \r\n  select(CharacterName, TransactionType, Type, Memo, Value) %>%\r\n  group_by(CharacterName) %>% \r\n  mutate(Value = abs(Value)) %>%\r\n  summarise(EP_Decay = sum(Value[TransactionType %in% \"EpDecay\"])) %>%\r\n  left_join(items_by_char, by = \"CharacterName\") %>% \r\n  mutate(Ratio = round(EP_Decay/Items_Claimed_GP, 2)) %>% \r\n  filter(Ratio > 0) %>% \r\n  slice_max(order_by = Ratio, n = 50) %>%\r\n  ggplot(aes(x = fct_reorder(CharacterName, Ratio, .desc = TRUE), y = (Ratio), text = paste(\r\n    CharacterName, \"lost\", Ratio, \"EP to decay per\", Items_Claimed_GP, \"item(s)\"))) +\r\n  geom_col(position = \"dodge\") +\r\n  theme(axis.text.x = element_text(angle = 75)) +\r\n    labs(\r\n    title = \"EP Decay per Item Claimed for GP\",\r\n    x = \"Character\",\r\n    y = \"EP Decay/Items Claimed\"\r\n  ), tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nThis chart really speaks to the impact of decay on EPGP. The ratio of EP decay / numbers of items claimed for GP gives you an idea of how the 20% decay impacts your EP values. Now to be clear, I’m not faulting anyone for ‘hoarding’ EP or not claiming items during raids. If a player has all the loot they need and they continue to raid, they are supporting the guild. However, holding a lot of EP and claiming less items seems to have a larger impact because of decay.\r\nEffort Points and Gear Points\r\nUp to this point, we’ve briefly discussed EPGP as a loot distribution system. In order to gain EP, a player needs to either attend guild events or contribute in-game materials to the guild. These in-game materials require a player to spend time gathering the item or spend gold to buy the item and donate it to the guild. We’ve looked at overall EP and GP values and some ways players spend GP, so let’s break down some of the ways players gain EP:\r\n\r\n\r\nggplotly(\r\n  t_actions %>%\r\n  filter(TransactionType %in% c(\"EpAutomated\", \"EpManual\")) %>% \r\n  select(CharacterName, Value, TransactionType) %>%\r\n  group_by(CharacterName, TransactionType) %>%\r\n  summarise(Value = sum(Value)) %>%\r\n  group_by(TransactionType) %>%\r\n  slice_max(order_by = Value, \r\n            n = 20,\r\n            with_ties = TRUE) %>%\r\n  ggplot(aes(x = fct_reorder(CharacterName, Value, .desc = TRUE), \r\n             y = Value, \r\n             fill = TransactionType, \r\n             text = paste(Value, \"total EP awarded for\", TransactionType))) +\r\n  geom_col(position = \"dodge\") +\r\n  theme(axis.text.x = element_text(angle = 75)) +\r\n    labs(\r\n    title = \"Top EP Earned By Character\",\r\n    x = NULL,\r\n    y = \"EP Amount\"\r\n    ), tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nWell now, we’re finally getting into some of the juicy details. The chart above shows the top 20 characters for EpAutomated and EpManual EP types. Now before I go any further, for any of my fellow guild members reading this and feeling #exposed, just a reminder a benefit of EPGP is transparency, this data is available to anyone. Alright, now lets start throwing some shade :D\r\nOur guild’s resident WoW celebrity, Kyx, or better known by her full name SixPacKyx, tops the list at #1 for most EP earned for attending guild raids (remember EpAutomated is awarded for attending raid events) since July. Gintama, aka the weapon hoarder, aka Mr Afk, ironically is in the top 20 for EP earned by raiding and also for EP earned by manual entry, meaning he contributes a lot of resources to the guild. In the prior EPGP Values chart, Gintama clearly had the most EP overall but we now see that a large portion of his EP was a result of guild contributions. The characters in the middle of the pack, from Baxterwar to Arkadus, were also an interesting find. These characters were in the top 20 for both EP earned by raiding and EP earned by donating resources. The remaining 9 characters accrued more EP from contributions than the rest of the guild but did not earn as much EP for raiding.\r\nRollin’ for Loot \r\nAlright, the last data point I want to investigate is the EPGP [Roll] function. The [Roll] function is considered a new feature of the bot and represents a change in how characters can obtain loot. The reasoning goes like this: when Blizzard released “Phase 5” content for Classic Wow - essentially a patch or release of new content\" - epic items looted during raids which were less desirable to players became useful for the guild. Before Phase 5, we would give out gear which no players wanted to spend GP on by letting players “/roll” for the item. With the release of this Phase 5, items rolled on would now cost GP because, in this phase, epic items can be broken down (‘disenchanted’) into expensive and useful materials. This is probably not worth explaining in anymore length, but this resulting change meant that the guild would begin charge players GP based on the market rate of the material which the epic item disenchanted into. And in order to keep track of this for EPGP accounting, the [Claim] and [Roll] commands were created. (Since this command is new to Phase 5 content, all prior entries of an item will be considered as [Claim]s.)\r\n\r\n\r\nggplotly (\r\n  t_actions %>%\r\n  select(TransactionDate, TransactionType, Type, Memo) %>%\r\n  group_by(TransactionDate) %>%\r\n  filter(TransactionType == \"GpFromGear\") %>% \r\n  summarise(Rolls = sum(Type == \"Roll\"),\r\n            Items = n()) %>% \r\n  mutate(roll_pop = scales::percent(round(Rolls/(Rolls+Items), 1))) %>% \r\n  ggplot() + \r\n  geom_col(aes(x = TransactionDate, y = Items, text = paste(Items, \"items were claimed for GP on\", TransactionDate))) +\r\n  geom_point(aes(x = TransactionDate, y = Rolls, text = paste(Rolls, \"items were rolled for GP on\", TransactionDate,\r\n                                                              \"<br>\",roll_pop, \"of loot went to rolls\"))) + \r\n  geom_line(aes(x = TransactionDate, y = Rolls)) +\r\n    labs(\r\n    title =  \" 'Roll' Popularity Over Time\",\r\n    x = \"Raid Date\",\r\n    y = \"Number of Items Dropped\"\r\n    ) + \r\n  scale_x_date(breaks = scales::pretty_breaks(15)),\r\n  tooltip = \"text\"\r\n)\r\n\r\n\r\n\r\n\r\nWell it would appear that ‘Rolls’ have not drastically impacted our EPGP system and they haven’t really changed much over time. I think as more items drop and as more players get geared, we may see loot going to rolls more often but we could also see more alts attending raids and claiming items for GP. It may work itself out. Either way, rolling for loot would seems to hold steady at around 20-30% of total drops per raid night.\r\nThere is certainly more analysis I could do with this data, such as looking at DiscordUserID looting behaviors instead of individual characters, visualizing EPGP and PR over time, and there might even be enough statistical power in this data to run some regressions. For now, I’m going to stop and move on to a Tableau Public Dashboard that will facilitate more interaction and visualizations of our guild’s EPGP data. Maybe by the time Phase 6 is released and there are a few more months of data, I can return and run these charts again to see any changes.\r\n\r\nTableau Public Interactive Infographic\r\n\r\n\r\n\r\nClosing Up\r\nAlright, that’s going to do it for me with this post. I really enjoyed splitting my time between R and Tableau and I’ve learned a ton of really useful but specific information on a video game I play, so there’s that. Ha!\r\nHuge thanks and credit to Matt Fuqua - our Buzz Bot architect. Without the Buzz Bot, I would not have had such a rich and useful dataset for this post! Matt’s work can be found on his github.\r\n1 World of Warcraft Vanilla citations from Wikipedia: https://en.wikipedia.org/wiki/World_of_Warcraft\r\n2 The EPGP language and formulas were written by Matt in his Buzz Bot documentation\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-28-wow-buzz-analysis/wow_img.png",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2020-10-16-ymh-analysis-p2/",
    "title": "Your Mom's House Analysis, Part II",
    "description": "Part II of a II part analysis of YMH YouTube transcription data",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2020-10-16",
    "categories": [
      "data visualization",
      "R",
      "ggplot2",
      "interactive",
      "text analysis",
      "wordcloud",
      "nsfw"
    ],
    "contents": "\r\n\r\nContents\r\nSetup and data work\r\nDescriptivesEpisode length\r\nGuests\r\n\r\nText AnalysisWord Usage\r\nWordcloud\r\nPotty Mouths\r\n\r\nCatch Phrase Traceback\r\n\r\nI’m back, Jeans! For those who missed Part I of this two-part YMH analysis, I followed along with Eric Ekholm’s blog post to pull YouTube speech-to-text captions for a playlist of YMH videos. If you missed the warning, this is NSFW content so tread lightly all you cool cats and kittens.\r\nNow that we have the YouTube text data, it’s time to dive in and see what we can learn about our pals Tom and Christine. Similar to Part 1’s blog post, I am going to follow along with Eric’s second YMH blog post where he analyzed this data and find opportunities to learn from him as well as unpack the data in other ways.\r\nBefore we get off on this analysis, there are a few important observations of the text data I should mention:\r\nThe episode transcripts are automatically created by Youtube’s speech-to-text model. It usually performs very well, but it does make some mistakes & obviously has trouble with proper nouns (e.g. TikTok).\r\nThe speakers of each phrase are not tagged or timestamped. That is, there are no indications whether Tom, Christina, or someone else are speaking any given line.\r\nThis should be fairly obvious at this point but I’m only looking at the episodes available on Youtube. The most recent episode included is episode 572 with Chris Distefano, aka Chrissy Drip Drop, aka Chrissy one-and-done, aka Chrissy vodka soda.\r\nAlright we’ve feathered it enough, let’s get started with some data cleaning and manipulation.\r\nSetup and data work\r\n\r\n\r\nlibrary(tidyverse) \r\nlibrary(lubridate) \r\nlibrary(tm)         ## text mining\r\nlibrary(tidytext)   ## text mining\r\nlibrary(tidylo)     ## improved weights to lists, like top_n\r\nlibrary(igraph)     ## edge sequencing\r\nlibrary(ggraph)     ## ggplot2\r\nlibrary(vroom)      ## improved csv reading & in line piping\r\nlibrary(reactable)  ## interactive tables\r\nlibrary(plotly)     ## interactive plots!!\r\nlibrary(wordcloud)\r\n\r\n\r\nymh <- vroom(\"~/GitHub/blog_website_2.0/blog_data/ymh_data.csv\")\r\n\r\nymh_single_row <- ymh %>% \r\n  group_by(title, ep_no) %>%\r\n  summarise(text = str_c(text, collapse = \" \") %>%\r\n              str_to_lower()) %>%\r\n  ungroup()\r\n\r\n\r\n\r\nDescriptives\r\nEpisode length\r\nWe have a total of 175 observations, or episodes, worth of data to investigate, ranging from episode 345 to 572. Let’s take a look at some descriptives about YMH episodes.\r\n\r\n\r\nep_length <- ymh %>%\r\n  group_by(ep_no) %>%\r\n  summarise(minutes = (max(start) + max(duration))/60) %>%\r\n  ungroup()\r\n\r\nggplotly(ep_length %>%\r\n  ggplot(aes(x = ep_no, y = minutes)) +\r\n  geom_point(color = \"black\") +\r\n  geom_line(color = \"black\") +\r\n  labs(\r\n    title = \"YMH Episode Length\",\r\n    x = \"Episode Number\",\r\n    y = \"Length (mins)\"\r\n  )\r\n  )\r\n\r\n\r\n\r\n\r\nIt looks like we have an outlier episode. If you hover over that point on the plot with your mouse/cursor (yes, these plots are interactive!), you will see there is a break between in subtitles generated by YouTube between approx. episode 330 and 394. Let’s go ahead and remove all episodes below 394 moving forward and try this again\r\n\r\n\r\nymh <- ymh %>% filter(ep_no > 393)\r\n\r\nep_length <- ymh %>%\r\n  group_by(ep_no) %>%\r\n  summarise(minutes = (max(start) + max(duration))/60) %>%\r\n  ungroup()\r\n\r\nggplotly(\r\n  ep_length %>%\r\n  ggplot(aes(x = ep_no, y = minutes)) +\r\n  geom_point(color = \"black\") +\r\n  geom_line(color = \"black\") +\r\n  labs(\r\n    title = \"YMH Episode Length\",\r\n    x = \"Episode Number\",\r\n    y = \"Length (mins)\"\r\n  )\r\n  )\r\n\r\n\r\n\r\n\r\nThat’s much better!\r\nLooking at the second plot, it’s clear that around episode 450, the length of episodes began to increase to upwards of 250 minutes long (4 hours!) before plateauing at around 150 minutes. It’s interesting to see such wide variability in episode length, instead of more stepped/staggered changes. There are a lot of lows and highs among episode lengths, symptomatic of a person on Meth as Dr Drew would point out. With this much variability, let’s fit a smooth line to the plot to help identify more trends.\r\n\r\n\r\nggplotly(\r\n  ep_length %>%\r\n  ggplot(aes(x = ep_no, y = minutes)) +\r\n  geom_point(color = \"black\") +\r\n  geom_line(color = \"black\") +\r\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", color = \"blue\") +\r\n  labs(\r\n    title = \"YMH Episode Length, Second Attempt\",\r\n    x = \"Episode Number\",\r\n    y = \"Length (mins)\"\r\n  )\r\n  )\r\n\r\n\r\n\r\n\r\nAh, that looks better. So it looks like episode length hit it’s peak around episode 480 and smooths out at approximately ~150 minutes.\r\nGuests\r\nGuests, usually fellow comedians, are a huge part of the show. Let’s look into the official guests on the show. I say official because I am reliant upon the title to inform me who was a guest on the show. Regular characters like Tom’s parents, Dr Drew, and Josh Potter won’t show up in this figure.\r\n\r\n\r\nymh %>%\r\n  distinct(ep_no, .keep_all = TRUE) %>%\r\n  mutate(guests = str_replace_all(guests, \"\\\\&\", \",\")) %>%\r\n  separate_rows(guests, sep = \",\") %>%\r\n  mutate(guests, str_trim(guests)) %>%\r\n  filter(!is.na(guests)) %>%\r\n  count(guests, name = \"num_appearances\", sort = TRUE) %>%\r\n  filter(num_appearances > 1) %>%\r\n  reactable()\r\n\r\n\r\n\r\n\r\nAlright so not a big surprise to anyone that Tom’s best friend, Bert Kreischer, was the top guest here. I’m glad to see Nikki Glaser as number 3 on the list, for one reason because she is the only repeat female guest but she is also savagely hilarious. Overall, it’s pretty clear YMH does not have a lot of repeat guests (at least in in the most recent ~200 shows). You’ll notice I removed any guest who did not have more than 1 appearance to make the list more manageable.\r\nText Analysis\r\nWord Usage\r\nMoving past some of the descriptive data, lets get into the text analysis portion of this project - the result of all that YouTube API calling in Part I. I’ll be taking a lot cues from Eric’s blog post for this part since this will be my first time doing any serious textual analysis in R.\r\nJust like in the limitations of guest count based on title, if you look at the YouTube text-to-speech data, it is not structured to show who is speaking when it records text. No matter, as you’ll see there is a lot of rich text and data in here to dig through.\r\nThe first step is to remove many of the common words in the English language like “the”, “at”, “I’m”, etc.\r\n\r\n\r\nymh_words <- ymh_single_row %>%\r\n  filter(ep_no > 393)%>%\r\n  unnest_tokens(word, text) \r\n## creates a token for each word in the single row text \r\n## column we collapsed in the beginning of the post.\r\n\r\nymh_words <- ymh_words %>%\r\n  anti_join(stop_words) %>% \r\n  ## anti_join = ! in that it returns the opposite of what we want. \r\n  ## In this case, it returns all words that do not match with the stop_words \r\n  ## function containing the entire lexicon of English common words like \"the\"\r\nfilter(!(word %in% c(\"yeah\", \"gonna\", \"uh\", \"hey\", \"cuz\", \"um\", \"lot\"))) %>%\r\n  ## filtering out additional common YMH specific words\r\nfilter(!(is.na(word)))\r\n         \r\nggplotly(                     \r\n  ymh_words %>%               \r\n  count(word) %>%             \r\n  slice_max(order_by = n, n = 20) %>%             \r\n  ## select the top 20 rows and order them\r\n  ggplot(aes(x = n, y = fct_reorder(word, n))) +\r\n  geom_col(fill = \"red\") + \r\n  labs(\r\n    title = \"Most Used YMH Words\",\r\n    y = NULL,\r\n    x = \"Count\"\r\n  )\r\n  )\r\n\r\n\r\n\r\n\r\nYou have got to be __ ’ing kidding me. Turns out that somehow Eric was able to download the raw/uncensored YouTube captions back in May and I’m stuck here with this __ . When I noticed I wasn’t finding any curse words, I went back and watched a few YMH clips and noticed that every time they cursed, the YouTube captions transcribed __ on the video closed captions. And, as you can see from the chart, they’ve essentially aggregated all curse words into one huge cluster- __ . Let’s see if we can still work around this and even use it to our advantage.\r\nAside from YouTube censoring of our analysis, we have a few other notable mentions in the top 20 words used. “People” is clearing the most used word, but we also see “guy”, “cool”, “crazy”, and “music”. YMH does have some incredibly talented fans with their music submissions to close out the episode, Hendawg for example, so “music” makes a lot of sense.\r\nWordcloud\r\nThe bar chart was a little boring and is limited to the top 20 words. Let’s look at a more nuanced text analysis visualization: Wordclouds. Turns out the {wordcloud} package makes these things trivial to recreate with a bit of formatting.\r\n\r\n\r\nymh_wc <- ymh_words %>%\r\n  count(word) %>% filter(!grepl(\"[[a-z]]+\", word))\r\n\r\nymh_wc$word <- removePunctuation(ymh_wc$word) \r\n## Our curse word symbol \"__\" won't look right in the wordcloud so I opted to remove it\r\n  \r\nymh_wc_p <- wordcloud(words = ymh_wc$word, freq = ymh_wc$n, min.freq = 1,\r\n            max.words = 300, random.order = FALSE,\r\n            colors = brewer.pal(8, \"Set1\"))\r\n\r\n\r\n\r\n\r\nEven though we had to remove __ from the wordcloud (I could have replaced it with “curses” but we would just have a huge “curses” in the middle which isn’t very insightful), there are still some interesting words in here. One thing is for sure, I could increase the max.words() even higher than 300 and the takeaway here woudls till be that YMH has a wide vocabulary - not surprising for a podcast with almost 600 episodes.\r\nPotty Mouths\r\nGiven that YouTube seems to have provided us with a curse word catch-all token, let’s see if we can use this to our advantage. In Eric’s blog post, he looked into number of “fuckings” per minute by episode. We can do the same thing here for __ .\r\n\r\n\r\n## quick note here: while writing this plot I kept getting \"Error in annotate: unused arguments\"\r\n## because both the {tlm} and {ggplot2} packages have annotate functions. To resolve that, I simply\r\n## force-called the ggplot2 function and it worked fine! \r\n\r\nggplotly(\r\n  ymh_words %>%\r\n  filter(word == \"__\") %>%\r\n  count(ep_no, word) %>%\r\n  left_join(ep_length, by = \"ep_no\") %>%\r\n  mutate(cpm = n/minutes) %>%\r\n  ggplot(aes(x = ep_no, y = cpm)) +\r\n  geom_text(aes(size = cpm), label = \"curses\", \r\n            show.legend = FALSE, color = \"black\") +\r\n    ggplot2::annotate(\"text\", x = 535, y = 3.39, \r\n                      label = \"- Ep. 494 w/ Joey 'Coco' Diaz\", \r\n                      hjust = 0, size = 3.5) +\r\n    ggplot2::annotate(\"curve\", x = 494, xend = 500, y = 3.2, \r\n                      yend = 3, curvature = .4) +\r\n  labs(\r\n    title = \"__'s per minute per YMH episode\",\r\n    x = \"Episode number\",\r\n    y = \"__'s per minute\"\r\n  )\r\n  )\r\n\r\n\r\n\r\n\r\nWoah so here’s a shocker! Episode 494 with Joey Diaz blows away the other episodes! Episode 494 had nearly 3.5 curse words per minute, 50% more than the next closest episode (ep. 451 w/ Chris D’Elia) for a total of 460 curse words in just 2 hours. Funny enough, episode 516 with Donnell Rawlings and Grant Cardone had the last number of curses, just 26 (looks like a little spec just after 500 on the x-axis).\r\nCatch Phrase Traceback\r\nYMH is known for their catch phrases. Whether its “try it out”, “feathering in”, or “water champ”, the true mommies have a lot of inside jokes. Let’s see if we can traceback these jokes to when they first made appearances on the show. (Note: since we do not have a full collection of YMH episodes and the earlier episodes were audio only, we’ll only be able to traceback more recent phrases. I doubt we’ll be able to find the origin of “Jeans” or “Mommies” in here).\r\n\r\n\r\nggplotly(\r\n  ymh_words %>%\r\n  filter(word == \"julia\") %>%\r\n  count(ep_no, word) %>% \r\n  left_join(x = tibble(ep_no = unique(ep_length$ep_no), word = \"julia\"),\r\n            y = ., by = c(\"ep_no\", \"word\")) %>%\r\n  mutate(n = replace_na(n, 0)) %>%\r\n  ggplot(aes(x = ep_no, y = n)) + \r\n  geom_line(color = \"black\", size = 1.25) +\r\n    labs(\r\n    title = \"Good Morning, Julia!\",\r\n    subtitle = \"Counts of the word Julia per episode\",\r\n    x = \"Episode Number\",\r\n    y = \"Count\")\r\n  ) %>%\r\n  layout(title = list(text = paste0(\"Good Morning, Julia!\",\r\n                                    \"<br>\",\r\n                                    \"<sup>\",\r\n                                    \"Counts of the word Julia per episode\",\r\n                                    \"<\/sup>\")))\r\n\r\n\r\n\r\n\r\nThe word Julia clearly saw a limited run on YMH. It peaked, dramatically, on episode 478 and was first mentioned on episode 432. Julia was last used during episode 556 so it hasn’t seen any action, so to speak, for about 20 episodes. That spike in episode 478 is interesting, though! Checking Eric’s blog post, it looks like that episode had Tom and Christina actually interview Julia. That makes a lot of sense in how the show runs, they probably locked down the interview a few episodes prior and hyped it up by playing the video a lot, which would explain the increase on episode 468. Afterwards, the word essentially lost popularity after the hype died out in the following episodes.\r\nWe aren’t stop here. There are a plenty of other words or phrases to try to traceback with this data. Eric, in his blog post, wrote a neat little function to extract a bunch of words/phrases and create a faceted graph. Let’s try that out. (As a personal note, I did not write this function, Eric did. I put in some time and effort to understand and explain it in the code chunk+ below but all credit for this function belongs to Eric.)\r\n\r\n\r\n## create the function phrase\r\n## for vector \"nwords\", if the string count is 1 = word, else if it is not = 1 then = \"nram\"\r\n## (essentially a sequence of words, later identified as a phrase)\r\n## for \"ngrams\" split the ymh_single_row table into tokens, \"n = nwords\" for the phrases, else for the \"words\"\r\n## subset data in tpm which does not equal phrase function (i.e. does not equal the 'phrases' 'vector)\r\n## create the phrase_list using the map() function and the i_phrase function based on the phrases vector\r\n\r\ni_phrase <- function(phrase) {\r\n  nwords <- str_count(phrase, \" \") + 1\r\n  \r\n  token <- if (nwords == 1) {\r\n    \"words\"\r\n  } else {\r\n     \"ngrams\"\r\n  }\r\n  \r\n  tmp <- if (token == \"ngrams\") {\r\n    ymh_single_row %>%\r\n      unnest_tokens(output = words, input = text, token = token, n = nwords)\r\n  } else {\r\n    ymh_single_row %>%\r\n      unnest_tokens(output = words, input = text, token = token)\r\n  }\r\n  \r\n  tmp %>% \r\n    filter(words == phrase)\r\n}\r\n\r\nphrases <- c(\"julia\", \"feathering it\", \"tick tock\", \"let me eat\", \"hitler\", \r\n             \"ride or die\", \"sleep for three days\", \"face fart\", \r\n             \"new relationship energy\", \"cool guy\", \"milligram tom\", \"badly\")\r\n\r\nymh_phrase_list <- map(phrases, i_phrase)\r\n\r\nphrase_grid <- expand_grid(unique(ep_length$ep_no), phrases) %>%\r\n  rename(ep_no = 1)\r\n\r\nymh_phrase_df <- ymh_phrase_list %>%\r\n  bind_rows() %>%\r\n  count(ep_no, words) %>%\r\n  left_join(x = phrase_grid, y = ., by = c(\"ep_no\", \"phrases\" = \"words\")) %>%\r\n  mutate(n = replace_na(n, 0))\r\n\r\nggplotly(\r\n  ymh_phrase_df %>% \r\n  mutate(phrases = str_to_title(phrases)) %>%\r\n  ggplot(aes(x = ep_no, y = n)) +\r\n  geom_line(color = \"black\") +\r\n  facet_wrap(~phrases, scales = \"free_y\") +\r\n  labs(\r\n    y = \"\",\r\n    x = \"\",\r\n    title = \"Now There's a Cool Guy!\"\r\n    )\r\n)\r\n\r\n\r\n\r\n\r\nLot’s to unpack in here! First, I’ll point out that the y-axis is different for each plot. Synchronizing across all the y-axes would make it so some phrase counts would disappear entirely. Some interesting things I pulled out of this grid: “Cool Guy” spiked in usage over the past 175 episode during episode 570 with Ian Bagg. I’m guessing Christine had a particularly long ‘dark tock’ session. We also see some of Tom’s grammar in here, he consistently uses ‘badly’ throughout the playlist and particularly enjoyed using it word during episode 568. “Face fart”, one of my favorite phrases, is really dying out, with only 2 utterances lately. None of these words/phrases really compare with the ‘tick tock’ trend coming from the Christine corner. The tocks hit their peak during episode 511 with Ryan Sickler and Steven Randolph and seem consistent usage since.\r\nThat’s going to wrap it up for me and this analysis! This two-part blog post was a blast to work on! Not only did it refresh a lot of my {r} skills but I leanred a lot of new libraries and functions and, most importantly, it got me excited to keep blogging and working on projects!\r\nHope everyone learned something from this. Keep em High and Tight, Jeans!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-16-ymh-analysis-p2/ymh_p2_img.png",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {},
    "preview_width": 700,
    "preview_height": 450
  },
  {
    "path": "posts/2020-10-11-ymh-analysis-p1/",
    "title": "Your Mom's House Analysis, Part I",
    "description": "Part I of a II part analysis of YMH YouTube transcription data",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2020-10-11",
    "categories": [
      "data wrangling",
      "R",
      "text analysis",
      "youtube-dl",
      "nsfw"
    ],
    "contents": "\r\n\r\nContents\r\nLibraries\r\nTest YouTube API call, episode 571\r\nGenerate YMH URL list\r\nData Cleaning\r\nList API call\r\nFinalizing the API call output\r\n\r\nInspired by a fellow YMH podcast (warning NSFW) fan/mommy, Eric, I am trying my hand at analyzing YMH podcast transcripts. I plan on following along with Eric’s work flow below and diverting once I find an opportunity to personalize the analysis - aka keeping it high and tight and following proto.\r\nBased on the structure of Eric’s blog post, this analysis will proceed as follows:\r\nLoad R libraries\r\nTest API call\r\nGenerate YMH URL list\r\nData cleaning\r\nList API call\r\nFinalize API call output\r\nI don’t have any experience in pulling transcripts from YouTube or any serious text analysis so I’m excited to try it out. Thanks for not being stingy, Mark…. I mean Eric!\r\nLibraries\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(youtubecaption)\r\nlibrary(janitor)\r\nlibrary(lubridate)\r\nlibrary(reshape2)\r\n\r\n\r\n\r\nAside from {tidyverse}, {reshape2}, and {lubridate}, Eric uses {youtubecaption} for calling YouTube’s API to get the caption transcripts and {janitor} for some of the text analysis, namely the clean_names() function. These two packages are brand new to me so I’m excited to work with them.\r\nNote: I learned further into this analysis that {youtubecaption} relies on Anaconda, a comprehensive Python environment, to pull from YouTube’s API. The author of youtubecaption states the reason for this is due to the difficulty of calling YouTube’s API for the captain data in a user friendly format. Thus, this library acts as a handy and clean way of accessing YouTube’s API in Python (Anaconda) without needing to write a custom API call in another language. I didn’t have Anaconda on this machine so I had to take a break from the analysis to install Anaconda before running functions from that library. Save yourself some time and download it before getting started!\r\nTest YouTube API call, episode 571\r\n\r\n\r\nymh_e571 <- get_caption(\"https://www.youtube.com/watch?v=dY818rfsJkk\")\r\n\r\n\r\n\r\nAnd there we go, a really straight forward way to download a transcript from a specific YouTube video. I can see why Eric used this package and that’s the easiest API call I’ve ever used. Let’s take a look at what we are working with:\r\n\r\n\r\nhead(ymh_e571)\r\n\r\n\r\n# A tibble: 6 x 5\r\n  segment_id text                                     start duration vid        \r\n       <int> <chr>                                    <dbl>    <dbl> <chr>      \r\n1          1 oh snap there's hot gear                  0.64     5.12 dY818rfsJkk\r\n2          2 merch method dot com slash tom sagura     3.04     5.84 dY818rfsJkk\r\n3          3 check out all our new stuff               5.76     3.12 dY818rfsJkk\r\n4          4 these club owners will take advantage as  9.92     3.04 dY818rfsJkk\r\n5          5 you know                                 12.2      2.40 dY818rfsJkk\r\n6          6 if you don't stand your ground i don't   13.0      2.88 dY818rfsJkk\r\n\r\nUpon inspection, the YouTube API call gives us 5 columns and just over 4500 rows or data. ‘Segment_id’ would be our unique identifier for each segment, ‘text’ is obviously the speech-to-text captions we are after, ‘start’ appears to be the time stamp when the text is recorded, the value of ‘duration’ is a mystery at the moment but we’ll look into that later, and ‘vid’ is simply the YouTube HTML video id (the characters after “watch?v=” in every YouTube video address).\r\nThe next step would be to download the transcripts for a number of episodes, but so far we’ve only made an API call to YouTube using the exact URL for a specific video. How are we going to call a list of videos outside of manually looking up all 571 YMH videos one at a time? Eric figured out a nice solution on his blog and we’ll explore it next.\r\nGenerate YMH URL list\r\nWhat we can do to script out a large API call is take a YMH YouTube playlist and export that to a text file. From there, we have a full list of YMH videos, their names, and, most importantly, the URLs. There are plenty of web tools and extensions to export a YouTUbe data to a .csv and only require a quick Google search to find one. Eric used this tool which looks straight forward and conveinent. Once you have the list, we’ll need to clean and arrange the file to use when calling from the YouTube API.\r\nHouston, we have a problem:\r\n\r\nTurns out the web app Eric used to gather the YouTube playlist and episode information is no longer working. We will have to find other means of generating this list to make the API call. After a painstaking Sunday searching for alternative web apps and testing them, I was not able to find anything that fit the needs for this project. That is until I stumbled upon YouTube-dl, a command-line program created to download YouTube videos and/or audio tracks from YouTube and a host of other sites. Turns out this program can also be used to scrape the YouTube metadata of videos. It took some time (and reddit/r/youtube-dl threads) to figure out how to use the tool to generate a list of video titles and URLs from a playlist, but I come to you with the knowledge! So lets do this, shall we?\r\nFirst thing you’ll want to do is download the youtube-dl .exe windows application from their site and place it into a folder. Open a cmd window and set that folder as your directory directory. Lucky for you, I’ve done all the troubleshooting and testing of this tool. These are the commands I decided to use, mainly due to my limited knowledge of the windows command line.\r\nThis command will result in a long csv file, with the episode title and url id all in one column:\r\nyoutube-dl.exe --flat-playlist --get-title --ignore-config --get-id -o \r\n\"https://www.youtube.com/watch?v=%(id)s\" \"https://www.youtube.com/playlist?list=PL-i3EV1v5hLd9H1p2wT5ZD8alEY0EmxYD\",\r\n>ymh_playlist.csv\r\nInstead of dealing with the above output, I decided to just export two files, one with the playlist video titles and another with the playlist video url id’s and simply join them in R:\r\nyoutube-dl.exe --flat-playlist --get-title --ignore-config -o \r\n\"https://www.youtube.com/watch?v=%(id)s\" \"https://www.youtube.com/playlist?list=PL-i3EV1v5hLd9H1p2wT5ZD8alEY0EmxYD\",\r\n>ymh_title.csv\r\nyoutube-dl.exe --flat-playlist --get-id --ignore-config -o \r\n\"https://www.youtube.com/watch?v=%(id)s\" \"https://www.youtube.com/playlist?list=PL-i3EV1v5hLd9H1p2wT5ZD8alEY0EmxYD\",\r\n>ymh_id.csv\r\nYou should end up with either one or two .csv’s with a single column of data, depending on your preferred method. Let’s take a look at them:\r\n\r\n\r\nplaylist_title <- read.csv(\"~/GitHub/blog_website_2.0/blog_data/ymh_title.csv\", \r\n                           header=FALSE)\r\nplaylist_url <- read.csv(\"~/GitHub/blog_website_2.0/blog_data/ymh_id.csv\", \r\n                         header=FALSE)\r\nhead(playlist_title)\r\n\r\n\r\n                                                      V1\r\n1  Your Mom's House Podcast - Ep. 572 w/ Chris Distefano\r\n2      Your Mom's House Podcast - Ep. 571 w/ Marcus King\r\n3         Your Mom's House Podcast - Ep. 570 w/ Ian Bagg\r\n4  Your Mom's House Podcast - Ep. 569 w/ Duncan Trussell\r\n5     Your Mom's House Podcast - Ep. 568 w/ Kevin Nealon\r\n6 Your Mom's House Podcast - Ep. 567 w/ Fortune Feimster\r\n\r\nhead(playlist_url)\r\n\r\n\r\n           V1\r\n1 wTvMDkAZZnM\r\n2 dY818rfsJkk\r\n3 _VCG6PzLnRA\r\n4 J4hQnfu9nkY\r\n5 bGklkg3m-6U\r\n6 zSRBGlyisno\r\n\r\nData Cleaning\r\nWe’ll do some reshaping and data cleaning next. Now I’m not the best person to explain data cleaning steps and I know there are probably simpler, faster, and more efficient ways of doing it but these worked for me. I’d suggest searching for some of the experts on {tidyverse} and {reshape2} to learn about it!\r\n\r\n\r\nappend_rows <- which(!(grepl(\"Your\", playlist_title$V1)))\r\ntitle_rows <- c(20, 20, 86, 98, 250, 251)\r\nplaylist_title[paste(title_rows, sep = \",\"),] <- paste(playlist_title[paste(\r\n  title_rows, sep = \",\"),], \",\",playlist_title[paste(\r\n    append_rows, sep = \",\"),])\r\nplaylist_title <- playlist_title %>% filter((grepl(\"Your\", V1))) %>%\r\n  rename(\"title\" = V1)\r\nplaylist_url <- playlist_url %>% slice(1:245) %>% transmute(\r\n  url = paste(\"https://www.youtube.com/watch?v=\", V1, sep = \"\"))\r\nplaylist_clean <- cbind(playlist_title, playlist_url)\r\nplaylist_clean <- playlist_clean %>% \r\n  clean_names() %>%\r\n  ## systematically cleans any text or symbols up \r\n  separate(title, c(\"title\", \"episode\"), \"-\") %>%          \r\n  ## separate episode title by '-'\r\n  mutate(vid = str_replace_all(url, \".*=(.*)$\", \"\\\\1\"))    \r\n  ## creates a vid column of just the youtube short link, this column matches \r\n  ## the \"vid\" output from get_caption() and will be used to join the two tables\r\nglimpse(playlist_clean)\r\n\r\n\r\nRows: 245\r\nColumns: 4\r\n$ title   <chr> \"Your Mom's House Podcast \", \"Your Mom's House Podcast \", \"...\r\n$ episode <chr> \" Ep. 572 w/ Chris Distefano\", \" Ep. 571 w/ Marcus King\", \"...\r\n$ url     <chr> \"https://www.youtube.com/watch?v=wTvMDkAZZnM\", \"https://www...\r\n$ vid     <chr> \"wTvMDkAZZnM\", \"dY818rfsJkk\", \"_VCG6PzLnRA\", \"J4hQnfu9nkY\",...\r\n\r\nSuccess! We have 245 rows of observations of YMH episodes, #330 to #572, plus some extra/live episodes with titles and URLs. You’ll notice I tweaked some rows and cut off some rows in the cleaning steps. If you interrogate the data more, you’ll find that either the youtube-dl tool or the command line does not export data into a .csv. cleanly. I found some of the long YouTube titles ended up having part of the title placed on a second row. Also, the last 2 videos in the playlist are marked as deleted and/or private so I removed those as well.\r\nNow we can start working on pulling the transcripts for the 245 episodes we’ve generated.\r\nList API call\r\nThe next steps is to again use the get_caption() function again but this time we need the function to pull all the captions from the list of URLs. We also want to make sure the get_caption() function successfully runs through the list and alerts us to any video it cannot successfully call. The way Eric did this was to wrap the get_caption() function inside the safely() function from {purrr}.\r\n\r\n\r\n# ?safely to read up                       \r\nsafe_cap <- safely(get_caption)\r\n\r\n\r\n\r\nBased on the help documents, safely() provides an enhanced output of errors, messages, and warnings outside of the base r reporting. This function should help determine if our large API call has an errors while running.\r\nWithin the same {purrr} library, the map() function applies a function to each element of a list. Therefore, in out case, map() will apply the get_caption() function to each URL in the playlist and, by wrapping safely() around get_caption(), will provide enhanced outputs and errors.\r\n\r\n\r\nplaylist_call <- map(playlist_clean$url,\r\n                     safe_cap)\r\n\r\n## warning - large print out:\r\n## glimpse(playlist_call)\r\n\r\n\r\n\r\nFinalizing the API call output\r\nWith the list API call finished, the return of the map() function resulted in a vector of the 245 captions. But instead of a single output, we have a result output and an error output from the safely() function and we want to get the ‘result’ outputs from each vector. (By the way, feel free to check those safely() error messages before continuing to see if you need to traceback anything).\r\nTo do this, we will again call the map() function as well as the pluck() function from {purrr} (you got to love these function names from a library called {purrr} :D). Pluck() essentially indexes a data structure and returns the desired object, in our case the resulting caption text from each URL.\r\n\r\n\r\nplaylist_data <- map(1:length(playlist_call),\r\n                     ~pluck(playlist_call, ., \"result\")) %>%\r\n                       compact() %>%\r\n                       bind_rows() %>%\r\n                       inner_join(x = playlist_clean,\r\n                                  y = .,\r\n                                  by = \"vid\")\r\n\r\nglimpse(playlist_data)\r\n\r\n\r\nRows: 527,189\r\nColumns: 8\r\n$ title      <chr> \"Your Mom's House Podcast \", \"Your Mom's House Podcast \"...\r\n$ episode    <chr> \" Ep. 572 w/ Chris Distefano\", \" Ep. 572 w/ Chris Distef...\r\n$ url        <chr> \"https://www.youtube.com/watch?v=wTvMDkAZZnM\", \"https://...\r\n$ vid        <chr> \"wTvMDkAZZnM\", \"wTvMDkAZZnM\", \"wTvMDkAZZnM\", \"wTvMDkAZZn...\r\n$ segment_id <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1...\r\n$ text       <chr> \"i'm not chrissy drip drop anymore you're\", \"chrissy fiv...\r\n$ start      <dbl> 0.160, 1.599, 3.679, 5.279, 6.160, 8.080, 9.280, 11.679,...\r\n$ duration   <dbl> 3.519, 3.680, 2.481, 2.801, 3.120, 3.599, 5.600, 8.641, ...\r\n\r\nSuccess! We’ve ended up with a table of 8 variables and over 500,000 segments of caption text. This is a good place to stop for now and we’ll pick back up on part 2 with the text analysis.\r\n\r\n\r\n\r\nThanks again to Eric Ekholm, blog post, who laid the foundation for me to learn this youtube api call process!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-11-ymh-analysis-p1/ymh_p1_img.png",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {},
    "preview_width": 707,
    "preview_height": 707
  },
  {
    "path": "posts/2017-06-14-chicago-ssl-scores/",
    "title": "Chicago SSL Scores",
    "description": "Exploratory Analysis of Chicago Police Department's SSL Scores",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2017-06-14",
    "categories": [
      "data science",
      "data visualization",
      "R",
      "ggplot2",
      "tidyverse",
      "public safety"
    ],
    "contents": "\r\n\r\nVisualizations with R\r\nToday I’m going to investigate some new data I found on the Chicago Data Portal. The Strategic Subjects List created and maintained by the Chicago Police Department is a list generated by an algorithm created by the Illinois Institute of Technology. The SSL List scores individuals on their probability of being involved in a shooting, either as the victim or offender, so the list is difficult to understand in its entirety because it groups victims and criminals together.\r\nTo start, let’s set our directory and load the .csv file, then load the libraries we will be using, in this instance it will be ggplot2 and tidyverse, both very powerful R libraries. Finally, let’s get a preliminary look at the data.\r\n\r\n\r\nsetwd(\"enter the path of your data here\")\r\n\r\n# Load SSL list data\r\nssl <- read.csv(\"ssl.csv\")\r\n\r\n# Load Libraries\r\nlibrary(ggplot2)\r\nlibrary(tidyverse)\r\n\r\n# Lets look at the data.frame\r\nhead(ssl)\r\nnames(ssl)\r\nsummary(ssl)\r\n\r\n\r\n\r\nWhat you should see in your console is a lot of information on the dataset we’re working with. For example, there are 48 columns, or variables, to work with as well as a variety of data types from strings to integers to binary data.\r\nLet’s jump right into the interesting part of this dataset, the SSL scores and graph the distribution of scores. It should be noted that this dataset, straight from the Chicago data portal, is a complete de-sensitized dataset of the SSL scores from its inception in August of 2012 through 2016.\r\n\r\n\r\n# Plot SSL Scores for Chicago\r\nplot1 <- ggplot(data = ssl, aes(x = SSL.SCORE)) + geom_histogram(binwidth = 5)\r\nplot1 + xlab(\"SSL Score 2012-2016\") + ylab(\"Count\") + labs(title = \"SSL Histogram 2012 to 2016\") + theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\nWe should see a relatively even distributed histogram of SSL scores for 2012 to 2016. It looks like most of the SSL scores are between 200 and 400 with a peak at approximately 325.\r\n\r\nChicago’s homicide rates and violent crime rates for 2016 are especially prolific in the media and within the city. Let’s look specifically at the 2016 SSL data. Fortunately, in the SSL dataframe is a column called “LATEST.DATE” which indicates the latest date by year of police contact. Let’s subset the data for 2016 and see if it differs from the overall data. First, we need to subset the data for 2016 , then we need to re-plot our new 2016 data.\r\n\r\n\r\n# subset just 2016\r\nSSL16 <- subset(ssl, LATEST.DATE == 2016)\r\n\r\n# plot SSL scores for 2016\r\nplot16 <- ggplot(data = SSL16, aes(x = SSL.SCORE)) + geom_histogram(binwidth = 5)\r\nplot16 + xlab(\"SSL Score 2016\") + ylab(\"Count\") + labs(title = \"SSL Histogram for 2016\") + theme(plot.title = element_text(hjust = 0.5))\r\n\r\n\r\n\r\nThis time, we get a new graph that is specific to the latest date of police contact for 2016. We definitely see some small changes in the histogram but the overall structure of the distribution is the same. I found it interesting that SSL scores equal to 500 saw a significantly large increase to the previous plot.\r\n\r\nThat’s what 2016 looks like, but how have SSL scores difference from 2012 to 2016? Well even though we do not have a great date for when SSL scores were entered, we can see when individuals with SSL scores were last in contact with the police as a proxy for SSL activity by date. For this, we’ll need to total up SSL subjects by year.\r\n\r\n\r\n# create a list of SSL latest dates by year\r\nssl_year <- table(ssl$LATEST.DATE)\r\nprint(ssl_year)\r\nssl_yr_table <- as.data.frame(ssl_year)\r\nssl_table <- ssl_yr_table[7:11, ]\r\nnames(ssl_table) <- c(\"Year\", \"Number_SSL_Subjects\")\r\n\r\n# plot\r\np_year <- ggplot(data = ssl_table, aes(x = Year, y = Number_SSL_Subjects)) \r\np_year + geom_point(aes(color = Year)) + xlab(\"Year\") + ylab(\"Number of SSL subjects\")\r\n\r\n\r\n\r\n\r\nIt appears that SSL contact by police has skyrocketed since 2012 and may have dropped a significant amount in 2016. However, for such a short time frame, not much interpretation is appropriate.\r\nlove looking at data in a spatial context. Therefore, let’s look at total SSL scores by community area to see if these scores are distributed within the communities we associate with high gang activity, homicides, and violence. Refer back to the column names in the data. We specifically had a variable called “COMMUNITY.AREA”, so we’ll use that for this plot\r\n\r\n\r\n# plot 2016 SSL scores for Chicago by Community Area\r\nc_area <- ggplot(data = SSL16, aes(x = COMMUNITY.AREA, y = SSL.SCORE)) + geom_col()\r\nc_area + theme(plot.title = element_text(hjust = 0.5)) + xlab(\"Community Area\") + ylab(\"SSL Score\") + labs(title = \"Community Area SSL scores\")\r\n\r\n\r\n\r\n\r\nWell this gives an ugly representation of the data for multiple reasons. First, the communities areas are smushed together along the x-axis, making them indistinguishable Also, it looks like we have an issue with the data, one area appears to constitute a huge majority of SSL scores. Let’s tackle this problem one step at a time. First, let’s adjust the graph to see community areas better by tilting the community area identifier.\r\n\r\n\r\n# plot 2016 SSL scores for Chicago by Community Area\r\nc_area_clean <- ggplot(data = SSL16_clean, aes(x = COMMUNITY.AREA, y = SSL.SCORE)) + geom_col()\r\nc_area_clean + theme(axis.text.x = element_text(angle = 90)) + theme(plot.title = element_text(hjust = 0.5)) + xlab(\"Community Area\") + ylab(\"SSL Score\") + labs(title = \"Community Area SSL scores\")\r\n\r\n\r\n\r\n\r\nMuch better, we can see all the community areas and their respective SSL scores. However, we still have this large column with no community area attached to it. Time to go back to the data.\r\n\r\n\r\nsummary(SSL16$COMMUNITY.AREA)\r\n\r\n\r\n\r\nA summary of community area provides us with a breakdown of the counts for each community area. Right away, we see a blank column ” ” that has over 32,000 SSL scores attached to it. That cannot be right, somewhere along the line CPD wasn’t able to attach a community area to these individuals. With any spatial reference, we need to remove them from the analysis. (If you look at the data, there are also some NA’s in the Community Area column so we’ll remove the empty values and NA values)\r\n\r\n\r\n# remove data with blank community area\r\nSSL16clean2 <- SSL16[!(is.na(SSL16$COMMUNITY.AREA) | SSL16$COMMUNITY.AREA == \" \"), ]\r\n\r\n\r\n\r\nUnfortunately, cleaning the data like this really limits our analysis. The total amount of observations for 2016 is reduced from 78,586 to 46,405 after removing SSL values without a spatial reference. However, that shouldn’t discourage us from continuing.\r\nLet’s finish by plotting the newly cleaned SSL scores by community area.\r\n\r\n\r\n# plot cleaned total 2016 scores for Chicago by Community Area\r\nc_area_clean2 <- ggplot(data = SSL16clean2, aes(x = COMMUNITY.AREA, y = SSL.SCORE)) + geom_col()\r\nc_area_clean2 + theme(axis.text.x = element_text(angle = 90)) + xlab(\"Community Area\") + ylab(\"SSL Score\") + labs(title = \"Community Area SSL scores\")\r\n\r\n\r\n\r\n\r\nWhat we end up with is a cleaned dataset of SSL scores, totaled, for each Chicago community area. From the graph, we can see that one community area in particular, Austin, is responsible for a large majority of SSL scores compared to all other community areas. The next closest community area appears to be West Garfield Park. If we go back to our summary of community areas, this makes sense because Austin holds the largest count of SSL subjects at 4122 and West Garfield Park has the second highest count of subjects at 2295.\r\nThis was a very basic exploratory analysis of some very important Chicago public safety. Hopefully, this post sheds some light on how powerful and quick plotting and graphing in R is, but more importantly that further analysis into CPD’s SSL scores is critical to understanding how to best utilize them.\r\nUntil next time,\r\nKevin\r\n\r\n\r\n\r\n",
    "preview": "posts/2017-06-14-chicago-ssl-scores/SSL12-16.jpeg",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2017-06-05-mapping-with-geoda/",
    "title": "Mapping with GeoDa",
    "description": "Exploratory Analysis of Census, Crime, and Call for Service data in Chicago",
    "author": [
      {
        "name": "Kevin Magnan",
        "url": "https://twitter.com/KevinMagnan"
      }
    ],
    "date": "2017-06-05",
    "categories": [
      "data science",
      "geoda",
      "mapping",
      "public safety"
    ],
    "contents": "\r\n\r\nContents\r\nExploratory Analysis\r\nSpatial Analysis\r\nConclusion\r\n\r\nExploratory Analysis\r\nChicago has historically been a segregated city and continues to be segregated in 2017. While the population of Chicago is roughly split into thirds for blacks, Hispanics, and whites, these populations, similar to crime rates, are not spatially distributed evenly within the city according to census data. These three populations display near perfect segregation, with black populations concentrated in the south and west communities, Hispanics concentrated in the northwest and southwest communities, and whites concentrated in the northern communities.\r\nFigure 1 Percent BlackFigure 2 Percent HispanicFigure 3 Percent WhiteRacial demographics are not the only characteristic that exhibits spatial variability; in light of racial segregation, educational attainment and average household income also fall in line with a socioeconomic segregation narrative. Figures 4, 5, and 6 visualize the extremes to which Chicago is segregated economically and with educational attainment. Specifically, higher household incomes are located in the same regions which are occupied by majority whites while lower household incomes are associated with black and Hispanic populations. These same economic disconnects are seen in age 25 and older population educational attainment.\r\nFigure 4 Average Household IncomeFigure 5 Percent of Population With Less Than High School EducationFigure 6 Percent of Population With High School EducationFinally, as was alluded to by the University of Chicago Crime Lab’s Gun Violence report as well as the Chicago Police Department and numerous Chicago media outlets, Chicago crime is disproportionately concentrated in these areas of high black and Hispanic populations with low educational attainment and low incomes. Violent crime rates in figure 7 and homicide rates in figure 9 bring further evidence to the claim that Chicago’s violent crimes and murders occur in primarily black and Hispanic neighborhoods with low income and education. Property crimes, in figure 8, tell a slightly different story since these crimes are more prolific in areas with more wealth and property value, like Chicago’s Loop area.\r\nFigure 7 Violent Crime Rate per 100kFigure 8 Property Crime Rate per 100kFigure 9 Homicide Rate per 100k311 call rates, represented in figure 10, present a different narrative than one would imagine in the context of Chicago’s crime rates and population demographics. As far as what can be taken at face value from these figures, there does not appear to be any patterns or clusters of 311 call rates in an area. If communities were to have high collective efficacy and be proactive in keeping their neighborhood in order, then we would expect the areas with either high violent and high property crimes or low violent and property crimes to be calling the city of Chicago at the highest rates. Instead, we see a dispersed rate of 311 calls incongruent to property crime locations and only partially reflective of violent crimes and homicides.\r\nFigure 10 311 Call Rate per 100k\r\nSpatial Analysis\r\nIn the exploratory analysis, I utilized quantile maps to visually investigate whether certain community area characteristics were clustering in specific areas of the city. To create a more scientific determination of clustering, I conducted global and local Moran I and Local Getis-Ord Gi* (G-i-star) analyses. As a precursor to these analyses, I experimented with three different spatial weight matrixes, queen contiguity, rook contiguity, and 5 nearest neighbors. Figures 11 and 11a show histograms of both the queen and rook contiguity weights (since nearest neighbor weights produced a flat graph it was not included); I determined that a rook spatial weight matrix was the best fit for the data since it most closely resembled a normal distribution and the connectivity map for the queen weights produced undesirable neighbors.\r\nFigure 11 Queen Contiguity Spatial WeightFigure 11a Rook Contiguity Spatial WeightTo determine spatial autocorrelation, that is how grouped similar and dissimilar values are to one another, global Moran’s I tests were ran on the variables presented in the exploratory analysis with rook contiguity as the weights matrix. To no surprise, racial demographics produced highly positive values: black = 0.71, Hispanic = 0.61, and white = 0.69. Similarly, average household income, percent of population with less than a high school education, and percent of population with a bachelor’s degree had high positive Moran’s Index values at 0.59, 0.56, and 0.72 respectively. Finally, criminal activity exhibited spatial autocorrelation in its Moran’s I values; Violent crime produced a 0.52 index, property crime produced a 0.37 index and homicides produced a 0.47 index. 311 call rate spatial autocorrelation was less pronounced, with a Moran’s I value of 0.39. Overall, we see spatial autocorrelation high for racial demographics and bachelor’s degrees at around a 0.7 Moran’s I value while income and low education producing slightly lower values and crimes even more. However, violent crimes are systematically more spatially autocorrelated than property crimes and 311 call rates exhibit the lowest spatial autocorrelation of the variables.\r\nGlobal Moran’s I is limited, producing results which only determine if the entire spatial study area is autocorrelated; therefore, in order to determine which community areas have spatial autocorrelation and clustering, local Moran’s I and Gi star tests were ran. Beginning with 311 call rates, figure 12 and 12a displays the results of these tests. We can see that statistically significant (p < 0.05) highly similar 311 call values and hot spots are largely present in the west community areas and 3 communities in the south. Meanwhile, dissimilar cold spots exist along the lake in communities like the Loop. For violent and property crimes, the local Moran’s I and Gi star tests show that the most northern communities represent dissimilar and cold locations in terms of crime rates. At the same time, violent and property crime hot spots and similar values are located in the areas we would expect, specifically the south and west for violent crimes (homicides matched these trends) and around the loop and some areas of the south for property crimes, see figures 13 and 14. It’s worth mentioning that average household income, less than high school education and bachelor’s degree local Moran I’s and Gi star’s similar/dissimilar and hot/cold spot values paralleled the findings from the exploratory analysis. In the interest of brevity, I decided not to include those maps.\r\nFigure 12 GI Star 311 Call RateFigure 12a Local Moran’s I 311 Call RateFigure 13 GI Star Violent Crime RateFigure 13a Local Moran’s I Violent Crime RateFigure 14 GI Star Property Crime RateFigure 14a Local Moran’s I Property Crime RateIn the final section of this analysis, I assessed the spatial regression models used in this analysis. Model 1 was an OLS regression model with 311 call rates as the dependent variable and the independent variables as violent crime, property crime, white, black, Hispanic, average household income, less than high school education, and bachelor’s degree incorporating the rook contiguity spatial weights matrix .\r\n\r\nBased on Model 1’s diagnostics for spatial dependence results in which the Robust LM (error) test was not significant, I determined that a spatial error test would be appropriate this Model 2.\r\n\r\nReviewing Table 1’s regression outputs, there is generally weak evidence that this model is effective in explaining the variation in Chicago’s 311 call rates. In Model 1, the only statistically significant independent variable is percent Hispanic with a positive coefficient of 124.28, suggesting that a larger Hispanic population is predictive of higher 311 call rates. Meanwhile, in the spatial error model we see violent crime, percent Hispanic, and percent with less than a high school education as statistically significant. This model gains more predictive power than Model 1 in its ability to predict how often a community area calls 311. Violent crime and percent Hispanic have positive coefficients while percent less than high school education has a negative coefficient, suggesting that higher crime rates and higher Hispanic populations are associated with greater 311 call rates while lower percentages of the population with less than high school education are associated with 311 call rates. Overall, both models perform poorly in predicting how often a community area calls 311 for local government services. Reviewing the R2 values for the two models, we see a small value for Model 1, 0.23, and nearly an increase of 50% for the spatial error model, 0.42. However, the majority of both models are not statistically significant so caution should be taken when considering the R2 values.\r\n\r\nConclusion\r\nThis report endeavored to show that Chicago’s segregation extends past racial and economic and that Chicago’s 2016 crime rates were heavily segregated within the city. Violent crime rates were clustered in areas with low socioeconomic indicators and high minority populations while property crimes were more spread out around the city in respect to racial demographics, but were more heavily centered near the loop which is a high economic area. In an attempt to determine if 311 call rates were impacted by these clustered variables, an OLS regression and a spatial error regression were ran. These models produced little predictive capacity for 311 call rates for the Chicago area in 2016.\r\nThere are many logical reasons that 311 call rates are difficult to predict. Namely, calling 311 is not only reflective of a neighborhoods collective efficacy and engagement but also a sign of how much an area trusts the police and the government to solve their problems. Also, 311 calls do not solely originate from residents and can be initiated by strangers, commuters, and tourist. Areas where these values are high or low may impact 311 call rates. 311 calling is also not as common as its 911 counterpart and residents either may not have knowledge about calling 311 or may instead call 911, greatly impacting this specific analysis. Finally, budgetary restrictions may impact how the city of Chicago groups 311 calls and responds to 311 calls.\r\nWhile this report did not produce a statistically significant model with predictive capabilities, I still believe 311 data provides a great deal of information to cities regarding their neighborhood issues as well as community engagement. Further research should look to join 311 and 911 call data in a spatial model to see if a more well-rounded analysis of total government service calling is impacted by crimes, racial, and socioeconomic factors. Additionally, these results may indicate that the city of Chicago should look into an educational program to ensure residents are aware of the benefits of calling 311 in their neighborhoods.\r\n\r\n\r\n\r\n",
    "preview": "posts/2017-06-05-mapping-with-geoda/table.png",
    "last_modified": "2024-05-31T20:56:23-05:00",
    "input_file": {},
    "preview_width": 1023,
    "preview_height": 811
  }
]
